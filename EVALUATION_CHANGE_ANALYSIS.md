# GSM8K Evaluation Change Analysis

## Summary

The **baseline accuracy dropped from ~19% to 4%** due to evaluation changes. This reveals that training is working correctly for log probability optimization, but the model isn't learning to generate correct answers.

## Timeline of Runs

### Run 1: 20251116_063412 (Successful Non-Markovian)
- **Config**: `markovian=false`, `actor_reward_weight=1.0`
- **Baseline (batch 0)**: **19.26%**
- **Final (batch 3200)**: **51.18%**
- **Evaluation method**: Actor generates answers (after beafc9d fix)
- **Result**: ‚úÖ **Works well**

### Run 2: 20251116_204952 (Successful Non-Markovian)
- **Config**: `markovian=false`, `actor_reward_weight=1.0`
- **Baseline (batch 0)**: **18.73%**
- **Final (batch 8200)**: **54.51%**
- **Evaluation method**: Actor generates answers (after beafc9d fix)
- **Result**: ‚úÖ **Works well**

### Run 3: 20251118_061529 (Failing Markovian) - CURRENT RUN
- **Config**: `markovian=true`, `actor_reward_weight=1.0`
- **Baseline (batch 0)**: **4.02%** ‚ö†Ô∏è **DROP OF 15 PERCENTAGE POINTS**
- **After training (batch 2000)**: **6.82%** ‚ö†Ô∏è **Still terrible**
- **Evaluation method**: Actor generates answers (after beafc9d fix)
- **Result**: ‚ùå **Completely broken**

## What Changed in Evaluation (Commit beafc9d - Nov 17, 2025)

### Before (Old `evaluate_gsm8k.py`)
```python
# Always used critic model for answer generation
with torch.no_grad():
    answer_outputs = critic_model.generate(...)
```

### After (New `evaluation.py`)
```python
# Use actor if actor_reward_weight > 0
actor_reward_weight = hyperparameters.get("actor_reward_weight", 0.0)
answer_model = actor_model if actor_reward_weight > 0 else critic_model

with torch.no_grad():
    answer_outputs = answer_model.generate(...)
```

## Why Baseline Dropped

**Before the fix**:
- Evaluation used **critic** (frozen base model) for answers
- Llama-3.1-8B baseline: ~12-19% on GSM8K (reasonable)
- Training: Actor learns to write CoTs that maximize P(answer|CoT) as scored by critic

**After the fix**:
- Evaluation uses **actor** when `actor_reward_weight > 0`
- Untrained actor baseline: **4%** on GSM8K (terrible)
- Training: Actor STILL optimizes P(answer|CoT), NOT answer generation accuracy

## The Core Issue: Training vs Evaluation Mismatch

### What Training Optimizes
```python
# In calculate_answer_log_probs() - line 586
actor_answer_logprobs, extracted_answers = calculate_answer_log_probs(
    reward_model,
    state.tokenizer,
    state.device,
    questions,
    reward_reasoning,
    answers,  # ‚Üê GOLD ANSWER STRING
    state.hyperparameters,
    include_question=include_question_in_reward,
)
```

**Training calculates**: P(gold_answer | CoT) 
- Passes gold answer string (e.g., "18")
- Calculates log probability
- Uses as reward signal
- **Never checks if actor can generate correct answers**

### What Evaluation Measures
```python
# In evaluate_model_generic() - line 652
answer_outputs = answer_model.generate(
    input_ids=answer_inputs.input_ids,
    ...
    do_sample=False,  # Deterministic
)
```

**Evaluation measures**: Accuracy of generated answers
- Actor generates its own answer
- Extracts numerical value
- Compares with gold
- **Completely different from training objective**

## Why Non-Markovian Works But Markovian Fails

### Non-Markovian (`markovian=false`)
Answer prompt includes question:
```
Question: Janet's ducks lay 16 eggs...
Reasoning: [CoT generated by actor]
Answer: 
```

The actor can "cheat" by:
1. Attending to question context
2. Using question info to generate answer
3. CoT matters less for final answer

**Result**: 51-54% accuracy ‚úÖ

### Markovian (`markovian=true`)
Answer prompt ONLY has CoT:
```
Question: <Redacted>
Reasoning: [CoT generated by actor]
Answer:
```

The actor MUST rely on CoT:
1. Cannot see original question
2. Must extract answer from own CoT
3. But training never taught it to do this

**Result**: 4-7% accuracy ‚ùå

## The Training Bug

Training has a fundamental mismatch:

1. **Reward signal**: P(gold_answer | CoT)
   - Encourages mentioning gold number in CoT
   - High reward when "18" appears in reasoning
   
2. **Evaluation metric**: Correctness of generated answer
   - Actually generate and extract answer
   - Check if equals gold

**Example of the disconnect**:
```
Question: "James runs 3 sprints 3 times/week, 60m each. Total?"
Gold: 540

During training:
  Actor generates CoT: "...540..." (contains gold number)
  Reward: P("540" | CoT) is HIGH ‚úÖ
  Gradient: Push model to include 540 in reasoning

During evaluation:
  Actor generates CoT: "...2160..."  
  Actor generates answer: "2160"
  Extracted: 2160
  Correct: NO (gold is 540) ‚ùå
```

## Evidence: Contains Answer Metric

From training logs:
- **Batch 0-100**: 50% of CoTs contain gold answer
- **Batch 2000-2100**: 67% of CoTs contain gold answer

The model learned to put gold numbers in reasoning (high training reward) but NOT to generate correct final answers (what evaluation measures).

## Why This Wasn't Caught Earlier

1. **Before beafc9d**: Evaluation used critic, which was more robust
2. **Non-Markovian mode**: Actor could rely on question context, masking the issue
3. **Training metrics looked good**: Loss improved, normalized rewards increased
4. **"Contains Answer" misleading**: High percentage suggested learning, but it's gaming the objective

## The Fix

The evaluation change (beafc9d) was **CORRECT** - it properly matches training mode. The issue is in **training**, which needs to:

### Option 1: Use Generated Answer Correctness (Recommended)
```python
if hyperparameters["task_type"] == "gsm8k" and extracted_answers is not None:
    gold_ints = [extract_answer(ans) for ans in answers]
    
    # Binary reward based on correctness
    correctness_rewards = torch.tensor([
        1.0 if pred == gold else -1.0
        for pred, gold in zip(extracted_answers, gold_ints)
    ], device=device)
    
    # Replace log probability with correctness
    actor_answer_logprobs = correctness_rewards
```

### Option 2: Hybrid Approach
```python
# Combine log prob (smooth gradients) + correctness bonus
correctness_bonus = torch.tensor([
    10.0 if pred == gold else 0.0
    for pred, gold in zip(extracted_answers, gold_ints)
], device=device)

actor_answer_logprobs = actor_answer_logprobs + correctness_bonus
```

## Files Involved

### Training
- `src/train.py` line 586-595: `calculate_advantages()` - passes gold answers
- `src/train.py` line 154-271: `calculate_answer_log_probs()` - computes but doesn't use extracted_answers
- `src/train.py` line 1177-1180: "Contains Answer" metric - misleading

### Evaluation
- `src/evaluation.py` line 627-630: Correctly chooses actor when actor_reward_weight > 0
- `src/evaluation.py` line 652-665: Generates and extracts answers

## Recommendations

1. **Immediate**: Fix training to use correctness rewards for GSM8K
2. **Testing**: Verify improvement in Markovian mode
3. **Metrics**: Add "training_answer_accuracy" based on extracted_answers
4. **Rename**: Change "Contains Answer" to "Gold Number in CoT" for clarity

## Conclusion

- ‚úÖ **Evaluation is correct** (beafc9d fix was right)
- ‚ùå **Training is broken** (optimizes wrong objective)
- üéØ **Root cause**: Training uses P(gold|CoT), evaluation uses generated answer correctness
- üîß **Solution**: Use extracted_answers for rewards in training

---

**Date**: November 18, 2025  
**Status**: Confirmed - Training/Evaluation Mismatch  
**Severity**: Critical for Markovian mode with actor_reward_weight > 0


