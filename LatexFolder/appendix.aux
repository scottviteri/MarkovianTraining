\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {A}Additional Performance Analysis}{1}{appendix.A}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:wikiloss}{{1a}{1}{Training progress on Wikipedia continuation task for Llama 8B, showing normalized improvement in next-token prediction across four independent runs}{figure.caption.1}{}}
\newlabel{sub@fig:wikiloss}{{a}{1}{Training progress on Wikipedia continuation task for Llama 8B, showing normalized improvement in next-token prediction across four independent runs}{figure.caption.1}{}}
\newlabel{fig:faith_mistral}{{1b}{1}{Perturbation effects on Mistral 7B arithmetic reasoning, showing three types of CoT modifications: digit changes, character deletions, and right truncation. Averaged over 4 PPO training runs}{figure.caption.1}{}}
\newlabel{sub@fig:faith_mistral}{{b}{1}{Perturbation effects on Mistral 7B arithmetic reasoning, showing three types of CoT modifications: digit changes, character deletions, and right truncation. Averaged over 4 PPO training runs}{figure.caption.1}{}}
\newlabel{fig:original_vs_llama}{{1c}{1}{Cross-model evaluation comparing how different models (Mistral, GPT2, and Phi 3.5 Mini Instruct) utilize Llama 8B's CoT on GSM8K. Results averaged across 3 training runs with smoothing window of 40}{figure.caption.1}{}}
\newlabel{sub@fig:original_vs_llama}{{c}{1}{Cross-model evaluation comparing how different models (Mistral, GPT2, and Phi 3.5 Mini Instruct) utilize Llama 8B's CoT on GSM8K. Results averaged across 3 training runs with smoothing window of 40}{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Additional performance analysis across different tasks and metrics. (a) Training performance on Wikipedia. (b) Perturbation analysis on arithmetic. (c) Cross-model evaluation on GSM8K.}}{1}{figure.caption.1}\protected@file@percent }
\newlabel{fig:additional_analysis}{{1}{1}{Additional performance analysis across different tasks and metrics. (a) Training performance on Wikipedia. (b) Perturbation analysis on arithmetic. (c) Cross-model evaluation on GSM8K}{figure.caption.1}{}}
\citation{yang-etal-2017-reference}
\citation{lyu2023faithful}
\citation{Joshi2024}
\citation{burns2024discovering}
\citation{Tian2023}
\citation{lin_truthfulqa2022}
\citation{Silver2016}
\citation{Silver2017}
\citation{christiano2021eliciting}
\@writefile{toc}{\contentsline {section}{\numberline {B}Truthfulness and Eliciting Latent Knowledge}{2}{appendix.B}\protected@file@percent }
\newlabel{app:truth}{{B}{2}{Truthfulness and Eliciting Latent Knowledge}{appendix.B}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Qualitative Analysis of Generated CoTs}{3}{appendix.C}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}After Training}{3}{subsection.C.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}Before Training}{3}{subsection.C.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {D}Case Study of Wikipedia Prediction}{4}{appendix.D}\protected@file@percent }
\newlabel{app:case}{{D}{4}{Case Study of Wikipedia Prediction}{appendix.D}{}}
\citation{christiano2023deepreinforcementlearninghuman}
\citation{bai2022constitutional}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.1}CoT after Training:}{5}{subsection.D.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.2}CoT before Training:}{5}{subsection.D.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.3}Actual Continuation:}{5}{subsection.D.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {E}Impact Statement}{5}{appendix.E}\protected@file@percent }
\newlabel{sec:ethics}{{E}{5}{Impact Statement}{appendix.E}{}}
\bibdata{aaai2026}
\bibcite{bai2022constitutional}{{1}{2022}{{Bai et~al.}}{{Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon, Chen, Olsson, Olah, Hernandez, Drain, Ganguli, Li, Tran-Johnson, Perez, Kerr, Mueller, Ladish, Landau, Ndousse, Lukosuite, Lovitt, Sellitto, Elhage, Schiefer, Mercado, DasSarma, Lasenby, Larson, Ringer, Johnston, Kravec, Showk, Fort, Lanham, Telleen-Lawton, Conerly, Henighan, Hume, Bowman, Hatfield-Dodds, Mann, Amodei, Joseph, McCandlish, Brown, and Kaplan}}}
\@writefile{toc}{\contentsline {section}{\numberline {F}Reproducibility Statement}{6}{appendix.F}\protected@file@percent }
\bibcite{burns2024discovering}{{2}{2023}{{Burns et~al.}}{{Burns, Ye, Klein, and Steinhardt}}}
\bibcite{christiano2021eliciting}{{3}{2021}{{Christiano et~al.}}{{Christiano, Cotra, and Xu}}}
\bibcite{christiano2023deepreinforcementlearninghuman}{{4}{2023}{{Christiano et~al.}}{{Christiano, Leike, Brown, Martic, Legg, and Amodei}}}
\bibcite{Joshi2024}{{5}{2024}{{Joshi et~al.}}{{Joshi, Rando, Saparov, Kim, and He}}}
\bibcite{lin_truthfulqa2022}{{6}{2022}{{Lin et~al.}}{{Lin, Hilton, and Evans}}}
\bibcite{lyu2023faithful}{{7}{2023}{{Lyu et~al.}}{{Lyu, Havaldar, Stein, Zhang, Rao, Wong, Apidianaki, and Callison-Burch}}}
\bibcite{Silver2016}{{8}{2016}{{Silver et~al.}}{{Silver, Huang, Maddison, et~al.}}}
\bibcite{Silver2017}{{9}{2017}{{Silver et~al.}}{{Silver, Hubert, Schrittwieser, Antonoglou, Lai, Guez, Lanctot, Sifre, Kumaran, Graepel, Lillicrap, Simonyan, and Hassabis}}}
\bibcite{Tian2023}{{10}{2023}{{Tian et~al.}}{{Tian, Mitchell, Yao, Manning, and Finn}}}
\bibcite{yang-etal-2017-reference}{{11}{2017}{{Yang et~al.}}{{Yang, Blunsom, Dyer, and Ling}}}
\bibstyle{plainnat}
\gdef \@abspage@last{7}
