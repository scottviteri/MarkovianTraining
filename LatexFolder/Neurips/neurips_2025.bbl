\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdin et~al.(2024)Abdin, Aneja, Awadalla, Awadallah, Awan, Bach, Bahree, Bakhtiari, Bao, et~al.]{abdin2024phi3technicalreporthighly}
Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar~Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, et~al.
\newblock Phi-3 technical report: A highly capable language model locally on your phone, 2024.
\newblock URL \url{https://arxiv.org/abs/2404.14219}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, et~al.]{NEURIPS2020_1457c0d6}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, et~al.
\newblock Language models are few-shot learners.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin, editors, \emph{Advances in Neural Information Processing Systems}, volume~33, pages 1877--1901. Curran Associates, Inc., 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}.

\bibitem[Burns et~al.(2023)Burns, Ye, Klein, and Steinhardt]{burns2024discovering}
Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt.
\newblock Discovering latent knowledge in language models without supervision.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=ETKGuby0hcs}.

\bibitem[Casper et~al.(2023)Casper, Rauker, Ho, and Hadfield-Menell]{raukur2022toward}
Stephen Casper, Tilman Rauker, Anson Ho, and Dylan Hadfield-Menell.
\newblock Sok: Toward transparent {AI}: A survey on interpreting the inner structures of deep neural networks.
\newblock In \emph{First IEEE Conference on Secure and Trustworthy Machine Learning}, 2023.

\bibitem[Chung et~al.(2015)Chung, Kastner, Dinh, Goel, Courville, and Bengio]{DBLP:journals/corr/ChungKDGCB15}
Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron~C. Courville, and Yoshua Bengio.
\newblock A recurrent latent variable model for sequential data.
\newblock \emph{CoRR}, abs/1506.02216, 2015.
\newblock URL \url{http://arxiv.org/abs/1506.02216}.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{cobbe2021gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.14168}.

\bibitem[DeepSeek-AI et~al.(2025)DeepSeek-AI, Guo, Yang, Zhang, Song, Zhang, Xu, et~al.]{deepseekai2025}
DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, et~al.
\newblock Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025.
\newblock URL \url{https://arxiv.org/abs/2501.12948}.

\bibitem[Geiger et~al.(2022)Geiger, Wu, Lu, Rozner, Kreiss, Icard, Goodman, and Potts]{geiger2022inducing}
Atticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah Goodman, and Christopher Potts.
\newblock Inducing causal structure for interpretable neural networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages 7324--7338. PMLR, 2022.

\bibitem[Geva et~al.(2022)Geva, Caciularu, Wang, and Goldberg]{geva2022transformer}
Mor Geva, Avi Caciularu, Kevin~Ro Wang, and Yoav Goldberg.
\newblock Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space.
\newblock \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 30--45, 2022.

\bibitem[Grabb et~al.(2024)Grabb, Lamparth, and Vasan]{Grabb2024.04.07.24305462}
Declan Grabb, Max Lamparth, and Nina Vasan.
\newblock Risks from language models for automated mental healthcare: Ethics and structure for implementation.
\newblock \emph{medRxiv}, 2024.
\newblock \doi{10.1101/2024.04.07.24305462}.
\newblock URL \url{https://www.medrxiv.org/content/early/2024/04/08/2024.04.07.24305462}.

\bibitem[Gu and Dao(2024)]{gu2024mamba}
Albert Gu and Tri Dao.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock In \emph{First Conference on Language Modeling}, 2024.
\newblock URL \url{https://openreview.net/forum?id=tEYskw1VY2}.

\bibitem[Gu et~al.(2022)Gu, Goel, and Ré]{gu2022efficientlymodelinglongsequences}
Albert Gu, Karan Goel, and Christopher Ré.
\newblock Efficiently modeling long sequences with structured state spaces, 2022.
\newblock URL \url{https://arxiv.org/abs/2111.00396}.

\bibitem[Gurnee and Tegmark(2024)]{gurnee2024language}
Wes Gurnee and Max Tegmark.
\newblock Language models represent space and time.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=jE8xbmvFin}.

\bibitem[Hu et~al.(2022)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2022lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lo{RA}: Low-rank adaptation of large language models.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=nZeVKeeFYf9}.

\bibitem[Karl et~al.(2017)Karl, Soelch, Bayer, and van~der Smagt]{karl2017deepvariationalbayesfilters}
Maximilian Karl, Maximilian Soelch, Justin Bayer, and Patrick van~der Smagt.
\newblock Deep variational bayes filters: Unsupervised learning of state space models from raw data, 2017.
\newblock URL \url{https://arxiv.org/abs/1605.06432}.

\bibitem[Krishnan et~al.(2015)Krishnan, Shalit, and Sontag]{krishnan2015deepkalmanfilters}
Rahul~G. Krishnan, Uri Shalit, and David Sontag.
\newblock Deep kalman filters, 2015.
\newblock URL \url{https://arxiv.org/abs/1511.05121}.

\bibitem[Lamparth and Reuel(2023)]{lamparth2023analyzing}
Max Lamparth and Anka Reuel.
\newblock Analyzing and editing inner mechanisms of backdoored language models, 2023.
\newblock URL \url{https://arxiv.org/abs/2302.12461}.

\bibitem[Lamparth et~al.(2024)Lamparth, Corso, Ganz, Mastro, Schneider, and Trinkunas]{lamparth2024human}
Max Lamparth, Anthony Corso, Jacob Ganz, Oriana~Skylar Mastro, Jacquelyn Schneider, and Harold Trinkunas.
\newblock Human vs. machine: Language models and wargames, 2024.
\newblock URL \url{https://arxiv.org/abs/2403.03407}.

\bibitem[Lanham et~al.(2023)Lanham, Chen, Radhakrishnan, Steiner, Denison, Hernandez, Li, Durmus, Hubinger, Kernion, Lukošiūtė, Nguyen, Cheng, Joseph, Schiefer, Rausch, Larson, McCandlish, Kundu, Kadavath, Yang, Henighan, Maxwell, Telleen-Lawton, Hume, Hatfield-Dodds, Kaplan, Brauner, Bowman, and Perez]{lanham2023measuring}
Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamilė Lukošiūtė, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam McCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared Kaplan, Jan Brauner, Samuel~R. Bowman, and Ethan Perez.
\newblock Measuring faithfulness in chain-of-thought reasoning, 2023.
\newblock URL \url{https://arxiv.org/abs/2307.13702}.

\bibitem[Lyu et~al.(2023)Lyu, Havaldar, Stein, Zhang, Rao, Wong, Apidianaki, and Callison-Burch]{lyu2023faithful}
Qing Lyu, Shreya Havaldar, Adam Stein, Li~Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch.
\newblock Faithful chain-of-thought reasoning, 2023.
\newblock URL \url{https://arxiv.org/abs/2301.13379}.

\bibitem[Meng et~al.(2022)Meng, Bau, Andonian, and Belinkov]{meng2022locating}
Kevin Meng, David Bau, Alex~J Andonian, and Yonatan Belinkov.
\newblock Locating and editing factual associations in {GPT}.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2022.

\bibitem[Nanda et~al.(2023)Nanda, Chan, Lieberum, Smith, and Steinhardt]{nanda2023progress}
Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt.
\newblock Progress measures for grokking via mechanistic interpretability.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=9XFSbDPmdW}.

\bibitem[Nye et~al.(2022)Nye, Andreassen, Gur-Ari, Michalewski, Austin, Bieber, Dohan, Lewkowycz, Bosma, Luan, Sutton, and Odena]{nye2022show}
Maxwell Nye, Anders~Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena.
\newblock Show your work: Scratchpads for intermediate computation with language models, 2022.
\newblock URL \url{https://openreview.net/forum?id=iedYJm92o0a}.

\bibitem[Rivera et~al.(2024)Rivera, Mukobi, Reuel, Lamparth, Smith, and Schneider]{rivera2024escalation}
Juan-Pablo Rivera, Gabriel Mukobi, Anka Reuel, Max Lamparth, Chandler Smith, and Jacquelyn Schneider.
\newblock Escalation risks from language models in military and diplomatic decision-making, 2024.
\newblock URL \url{https://arxiv.org/abs/2401.03408}.

\bibitem[Rumelhart et~al.(1986)Rumelhart, Hinton, and Williams]{rumelhart1986learning}
David~E. Rumelhart, Geoffrey~E. Hinton, and Ronald~J. Williams.
\newblock Learning representations by back-propagating errors.
\newblock \emph{Nature}, 323\penalty0 (6088):\penalty0 533--536, 1986.
\newblock \doi{10.1038/323533a0}.
\newblock URL \url{https://doi.org/10.1038/323533a0}.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, and Mansour]{policy_gradient}
Richard~S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function approximation.
\newblock In \emph{Proceedings of the 12th International Conference on Neural Information Processing Systems}, NIPS'99, page 1057–1063, Cambridge, MA, USA, 1999. MIT Press.

\bibitem[Turpin et~al.(2023)Turpin, Michael, Perez, and Bowman]{turpin2023language}
Miles Turpin, Julian Michael, Ethan Perez, and Samuel~R. Bowman.
\newblock Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=bzs4uPLXvi}.

\bibitem[Wang et~al.(2022)Wang, Variengien, Conmy, Shlegeris, and Steinhardt]{wang2022interpretability}
Kevin~Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt.
\newblock Interpretability in the wild: a circuit for indirect object identification in {GPT}-2 small.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, brian ichter, Xia, Chi, Le, and Zhou]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed~H. Chi, Quoc~V Le, and Denny Zhou.
\newblock Chain of thought prompting elicits reasoning in large language models.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=_VjQlMeSB_J}.

\bibitem[Zelikman et~al.(2022)Zelikman, Wu, Mu, and Goodman]{eric_star2022}
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman.
\newblock Star: Bootstrapping reasoning with reasoning.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh, editors, \emph{Advances in Neural Information Processing Systems}, volume~35, pages 15476--15488. Curran Associates, Inc., 2022.

\bibitem[Zelikman et~al.(2024)Zelikman, Harik, Shao, Jayasiri, Haber, and Goodman]{zelikman2024quietstar}
Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah~D. Goodman.
\newblock Quiet-star: Language models can teach themselves to think before speaking, 2024.
\newblock URL \url{https://arxiv.org/abs/2403.09629}.

\bibitem[Zhou et~al.(2023)Zhou, Zhou, Han, and Kambadur]{zhou2023understanding}
Dani Zhou, Enyu Zhou, Kevin Han, and Prashant Kambadur.
\newblock Understanding chain-of-thought in llms through information theory.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[{Åström, Karl Johan}({1965})]{proto_pomdp1965}
{Åström, Karl Johan}.
\newblock {Optimal Control of Markov Processes with Incomplete State Information I}.
\newblock {10}:\penalty0 {174--205}, {1965}.
\newblock ISSN {0022-247X}.
\newblock \doi{{10.1016/0022-247X(65)90154-X}}.
\newblock URL \url{{https://lup.lub.lu.se/search/files/5323668/8867085.pdf}}.

\end{thebibliography}
