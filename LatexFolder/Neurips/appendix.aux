\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{yang-etal-2017-reference}
\citation{lyu2023faithful}
\citation{Joshi2024}
\citation{burns2024discovering}
\citation{Tian2023}
\citation{lin_truthfulqa2022}
\citation{Silver2016}
\citation{Silver2017}
\citation{christiano2021eliciting}
\@writefile{toc}{\contentsline {section}{\numberline {A}Additional Performance Analysis}{1}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Truthfulness and Eliciting Latent Knowledge}{1}{appendix.B}\protected@file@percent }
\newlabel{app:truth}{{B}{1}{Truthfulness and Eliciting Latent Knowledge}{appendix.B}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:wikiloss}{{1a}{2}{Training progress on Wikipedia continuation task for Llama 8B, showing normalized improvement in next-token prediction across four independent runs}{figure.caption.1}{}}
\newlabel{sub@fig:wikiloss}{{a}{2}{Training progress on Wikipedia continuation task for Llama 8B, showing normalized improvement in next-token prediction across four independent runs}{figure.caption.1}{}}
\newlabel{fig:faith_mistral}{{1b}{2}{Perturbation effects on Mistral 7B arithmetic reasoning, showing three types of CoT modifications: digit changes, character deletions, and right truncation. Averaged over 4 PPO training runs}{figure.caption.1}{}}
\newlabel{sub@fig:faith_mistral}{{b}{2}{Perturbation effects on Mistral 7B arithmetic reasoning, showing three types of CoT modifications: digit changes, character deletions, and right truncation. Averaged over 4 PPO training runs}{figure.caption.1}{}}
\newlabel{fig:original_vs_llama}{{1c}{2}{Cross-model evaluation comparing how different models (Mistral, GPT2, and Phi 3.5 Mini Instruct) utilize Llama 8B's CoT on GSM8K. Results averaged across 3 training runs with smoothing window of 40}{figure.caption.1}{}}
\newlabel{sub@fig:original_vs_llama}{{c}{2}{Cross-model evaluation comparing how different models (Mistral, GPT2, and Phi 3.5 Mini Instruct) utilize Llama 8B's CoT on GSM8K. Results averaged across 3 training runs with smoothing window of 40}{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Additional performance analysis across different tasks and metrics. (a) Training performance on Wikipedia. (b) Perturbation analysis on arithmetic. (c) Cross-model evaluation on GSM8K.}}{2}{figure.caption.1}\protected@file@percent }
\newlabel{fig:additional_analysis}{{1}{2}{Additional performance analysis across different tasks and metrics. (a) Training performance on Wikipedia. (b) Perturbation analysis on arithmetic. (c) Cross-model evaluation on GSM8K}{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Perturbation analysis with uncertainty quantification. Same data as the main paper's perturbation analysis but with error bars showing one standard deviation around the mean effect. The shaded regions are calculated using Gaussian KDE over a sliding window, providing confidence intervals for the perturbation sensitivity measurements. Higher values indicate stronger reliance on CoT when the question is absent.}}{3}{figure.caption.2}\protected@file@percent }
\newlabel{fig:perturbation_with_uncertainty}{{2}{3}{Perturbation analysis with uncertainty quantification. Same data as the main paper's perturbation analysis but with error bars showing one standard deviation around the mean effect. The shaded regions are calculated using Gaussian KDE over a sliding window, providing confidence intervals for the perturbation sensitivity measurements. Higher values indicate stronger reliance on CoT when the question is absent}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Qualitative Analysis of Generated CoTs}{3}{appendix.C}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}After Training}{3}{subsection.C.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}Before Training}{4}{subsection.C.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {D}On Baselines for Faithful CoT}{5}{appendix.D}\protected@file@percent }
\newlabel{app:baselines_faithful_cot}{{D}{5}{On Baselines for Faithful CoT}{appendix.D}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.1}Baselines for Optimizing Informativeness}{5}{subsection.D.1}\protected@file@percent }
\newlabel{app:baselines_informativeness}{{D.1}{5}{Baselines for Optimizing Informativeness}{subsection.D.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.2}Baselines for Faithful Language Model Reasoning}{5}{subsection.D.2}\protected@file@percent }
\newlabel{app:baselines_faithfulness}{{D.2}{5}{Baselines for Faithful Language Model Reasoning}{subsection.D.2}{}}
\newlabel{eq:informativeness_objective}{{1}{5}{Baselines for Faithful Language Model Reasoning}{equation.1}{}}
\citation{deepseekai2025}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.3}Baselines for CoT Fragility}{6}{subsection.D.3}\protected@file@percent }
\newlabel{app:baselines_fragility}{{D.3}{6}{Baselines for CoT Fragility}{subsection.D.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Summary.}{7}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {E}Case Study of Wikipedia Prediction}{7}{appendix.E}\protected@file@percent }
\newlabel{app:case}{{E}{7}{Case Study of Wikipedia Prediction}{appendix.E}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E.1}CoT after Training:}{7}{subsection.E.1}\protected@file@percent }
\citation{christiano2023deepreinforcementlearninghuman}
\citation{bai2022constitutional}
\@writefile{toc}{\contentsline {subsection}{\numberline {E.2}CoT before Training:}{8}{subsection.E.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {E.3}Actual Continuation:}{8}{subsection.E.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {F}Impact Statement}{8}{appendix.F}\protected@file@percent }
\newlabel{sec:ethics}{{F}{8}{Impact Statement}{appendix.F}{}}
\bibdata{neurips_2025}
\bibcite{bai2022constitutional}{{1}{2022}{{Bai et~al.}}{{Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon, Chen, Olsson, Olah, Hernandez, Drain, Ganguli, Li, Tran-Johnson, Perez, Kerr, Mueller, Ladish, Landau, Ndousse, Lukosuite, Lovitt, Sellitto, Elhage, Schiefer, Mercado, DasSarma, Lasenby, Larson, Ringer, Johnston, Kravec, Showk, Fort, Lanham, Telleen-Lawton, Conerly, Henighan, Hume, Bowman, Hatfield-Dodds, Mann, Amodei, Joseph, McCandlish, Brown, and Kaplan}}}
\bibcite{burns2024discovering}{{2}{2023}{{Burns et~al.}}{{Burns, Ye, Klein, and Steinhardt}}}
\@writefile{toc}{\contentsline {section}{\numberline {G}Reproducibility Statement}{9}{appendix.G}\protected@file@percent }
\bibcite{christiano2021eliciting}{{3}{2021}{{Christiano et~al.}}{{Christiano, Cotra, and Xu}}}
\bibcite{christiano2023deepreinforcementlearninghuman}{{4}{2023}{{Christiano et~al.}}{{Christiano, Leike, Brown, Martic, Legg, and Amodei}}}
\bibcite{deepseekai2025}{{5}{2025}{{DeepSeek-AI et~al.}}{{DeepSeek-AI, Guo, Yang, Zhang, Song, Zhang, Xu, et~al.}}}
\bibcite{Joshi2024}{{6}{2024}{{Joshi et~al.}}{{Joshi, Rando, Saparov, Kim, and He}}}
\bibcite{lin_truthfulqa2022}{{7}{2022}{{Lin et~al.}}{{Lin, Hilton, and Evans}}}
\bibcite{lyu2023faithful}{{8}{2023}{{Lyu et~al.}}{{Lyu, Havaldar, Stein, Zhang, Rao, Wong, Apidianaki, and Callison-Burch}}}
\bibcite{Silver2016}{{9}{2016}{{Silver et~al.}}{{Silver, Huang, Maddison, et~al.}}}
\bibcite{Silver2017}{{10}{2017}{{Silver et~al.}}{{Silver, Hubert, Schrittwieser, Antonoglou, Lai, Guez, Lanctot, Sifre, Kumaran, Graepel, Lillicrap, Simonyan, and Hassabis}}}
\bibcite{Tian2023}{{11}{2023}{{Tian et~al.}}{{Tian, Mitchell, Yao, Manning, and Finn}}}
\bibcite{yang-etal-2017-reference}{{12}{2017}{{Yang et~al.}}{{Yang, Blunsom, Dyer, and Ling}}}
\bibstyle{plainnat}
\gdef \@abspage@last{10}
