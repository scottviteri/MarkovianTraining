@inproceedings{Vaswani+2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@misc{Silver2017,
  title={Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  year={2017},
  eprint={1712.01815},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  doi={10.48550/arXiv.1712.01815},
  url={https://doi.org/10.48550/arXiv.1712.01815},
  note={arXiv:1712.01815 [cs.AI]}
}

@inproceedings{lin_truthfulqa2022,
author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics},
title = {Truthful{QA}: Measuring How Models Mimic Human Falsehoods},
year = {2022},
url = {https://arxiv.org/abs/2109.07958},
note = {ACL 2022 (main conference)}
}

@inproceedings{
    wei2022chain,
    title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
    author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and brian ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou},
    booktitle={Advances in Neural Information Processing Systems},
    editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
    year={2022},
    url={https://openreview.net/forum?id=_VjQlMeSB_J}
}

@misc{lyu2023faithful,
      title={Faithful Chain-of-Thought Reasoning}, 
      author={Qing Lyu and Shreya Havaldar and Adam Stein and Li Zhang and Delip Rao and Eric Wong and Marianna Apidianaki and Chris Callison-Burch},
      year={2023},
      eprint={2301.13379},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2301.13379}
}

@inproceedings{
    turpin2023language,
    title={Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting},
    author={Miles Turpin and Julian Michael and Ethan Perez and Samuel R. Bowman},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://openreview.net/forum?id=bzs4uPLXvi}
}

@misc{bentham2024chainofthought,
      title={Chain-of-Thought Unfaithfulness as Disguised Accuracy}, 
      author={Oliver Bentham and Nathan Stringham and Ana Marasović},
      year={2024},
      eprint={2402.14897},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.14897}
}

@inproceedings{
    krishna2023post,
    title={Post Hoc Explanations of Language Models Can Improve Language Models},
    author={Satyapriya Krishna and Jiaqi Ma and Dylan Z Slack and Asma Ghandeharioun and Sameer Singh and Himabindu Lakkaraju},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://openreview.net/forum?id=3H37XciUEv}
}

@misc{lanham2023measuring,
      title={Measuring Faithfulness in Chain-of-Thought Reasoning}, 
      author={Tamera Lanham and Anna Chen and Ansh Radhakrishnan and Benoit Steiner and Carson Denison and Danny Hernandez and Dustin Li and Esin Durmus and Evan Hubinger and Jackson Kernion and Kamilė Lukošiūtė and Karina Nguyen and Newton Cheng and Nicholas Joseph and Nicholas Schiefer and Oliver Rausch and Robin Larson and Sam McCandlish and Sandipan Kundu and Saurav Kadavath and Shannon Yang and Thomas Henighan and Timothy Maxwell and Timothy Telleen-Lawton and Tristan Hume and Zac Hatfield-Dodds and Jared Kaplan and Jan Brauner and Samuel R. Bowman and Ethan Perez},
      year={2023},
      eprint={2307.13702},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2307.13702}
}

@inproceedings{NEURIPS2020_1457c0d6,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and others},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{korbak_pretraining2023,
    author = {Korbak, Tomasz and Shi, Kejian and Chen, Angelica and Bhalerao, Rasika and Buckley, Christopher L. and Phang, Jason and Bowman, Samuel R. and Perez, Ethan},
    title = {Pretraining language models with human preferences},
    year = {2023},
    publisher = {JMLR.org},
    abstract = {Language models (LMs) are pretrained to imitate internet text, including content that would violate human preferences if generated by an LM: falsehoods, offensive comments, personally identifiable information, low-quality or buggy code, and more. Here, we explore alternative objectives for pretraining LMs in a way that also guides them to generate text aligned with human preferences. We benchmark five objectives for pretraining with human feedback across three tasks and study how they affect the trade-off between alignment and capabilities of pretrained LMs. We find a Pareto-optimal and simple approach among those we explored: conditional training, or learning distribution over tokens conditional on their human preference scores given by a reward model. Conditional training reduces the rate of undesirable content by up to an order of magnitude, both when generating without a prompt and with an adversarially-chosen prompt. Moreover, conditional training maintains the downstream task performance of standard LM pretraining, both before and after task-specific finetuning. Pretraining with human feedback results in much better preference satisfaction than standard LM pretraining followed by finetuning with feedback, i.e., learning and then unlearning undesirable behavior. Our results suggest that we should move beyond imitation learning when pretraining LMs and incorporate human preferences from the start of training.},
    booktitle = {Proceedings of the 40th International Conference on Machine Learning},
    articleno = {722},
    numpages = {28},
    location = {<conf-loc>, <city>Honolulu</city>, <state>Hawaii</state>, <country>USA</country>, </conf-loc>},
    series = {ICML'23}
}

@misc{zelikman2024quietstar,
      title={Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking}, 
      author={Eric Zelikman and Georges Harik and Yijia Shao and Varuna Jayasiri and Nick Haber and Noah D. Goodman},
      year={2024},
      eprint={2403.09629},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.09629}
}

@inproceedings{eric_star2022,
 author = {Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {15476--15488},
 publisher = {Curran Associates, Inc.},
 title = {STaR: Bootstrapping Reasoning With Reasoning},
 volume = {35},
 year = {2022}
}

@inproceedings{
    hoffman2023training,
    title={Training Chain-of-Thought via Latent-Variable Inference},
    author={Matthew Douglas Hoffman and Du Phan and David Dohan and Sholto Douglas and Tuan Anh Le and Aaron T Parisi and Pavel Sountsov and Charles Sutton and Sharad Vikram and Rif A. Saurous},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://openreview.net/forum?id=a147pIS2Co}
}

@misc{nostalgebraist2020interpreting,
author = {Nostalgebraist},
title = {Interpreting {GPT}: The {L}ogit {L}ens},
year = {2020},
howpublished = {LessWrong},
url = {https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens}
}

@article{pomdp1998,
title = {Planning and acting in partially observable stochastic domains},
journal = {Artificial Intelligence},
volume = {101},
number = {1},
pages = {99-134},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00023-X},
url = {https://www.sciencedirect.com/science/article/pii/S000437029800023X},
author = {Leslie Pack Kaelbling and Michael L. Littman and Anthony R. Cassandra},
keywords = {Planning, Uncertainty, Partially observable Markov decision processes},
abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (mdps) and partially observable MDPs (pomdps). We then outline a novel algorithm for solving pomdps off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to pomdps, and of some possibilities for finding approximate solutions.}
}

@book{hopcroft2006moore,
author = {Hopcroft, John E. and Motwani, Rajeev and Ullman, Jeffrey D.},
title = {Introduction to Automata Theory, Languages, and Computation (3rd Edition)},
year = {2006},
isbn = {0321455363},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
address = {USA}
}

@misc{bai2022constitutional,
title={Constitutional {AI}: Harmlessness from {AI} {F}eedback},
author={Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosuite and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Jared Kaplan},
year={2022},
eprint={2212.08073},
archivePrefix={arXiv},
primaryClass={cs.CL},
doi={10.48550/arXiv.2212.08073},
url={https://arxiv.org/abs/2212.08073}
}

@article{proto_pomdp1965,
  author       = {{Åström, Karl Johan}},
  issn         = {{0022-247X}},
  language     = {{eng}},
  pages        = {{174--205}},
  publisher    = {{Elsevier}},
  series       = {{Journal of Mathematical Analysis and Applications}},
  title        = {{Optimal Control of Markov Processes with Incomplete State Information I}},
  url          = {{https://lup.lub.lu.se/search/files/5323668/8867085.pdf}},
  doi          = {{10.1016/0022-247X(65)90154-X}},
  volume       = {{10}},
  year         = {{1965}},
}

@inproceedings{
    hu2022lora,
    title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
    author={Edward J Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=nZeVKeeFYf9}
}

@misc{schulman2017proximal,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1707.06347}
}

@misc{touvron2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      note = {Version 2},
      url = {https://arxiv.org/abs/2307.09288v2}
}

@misc{christiano2021eliciting,
title={Eliciting latent knowledge: How to tell if your eyes deceive you},
author={Christiano, Paul and Cotra, Ajeya and Xu, Mark},
institution={Alignment Research Center},
year={2021},
month={December},
url={https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit}
}

@misc{viteri2021eliciting,
title={Abstraction, {A}gents, and {ELK}},
author={Scott Viteri},
institution={Stanford University},
year={2022},
month={January},
url={https://docs.google.com/document/d/1_Pd7KQohEGA-jXEV3c1df1yOCmmVZF-MvuBfSv_q-D0/edit#heading=h.rjos7vn28l74}
}

@misc{ling2017program,
title={Program Induction by Rationale Generation : Learning to Solve and Explain Algebraic Word Problems},
author={Ling, Wang and Yogatama, Dani and Dyer, Chris and Blunsom, Phil},
year={2017},
eprint={1705.04146},
archivePrefix={arXiv},
primaryClass={cs.AI},
url={https://arxiv.org/abs/1705.04146},
note={Version 3}
}

@inproceedings{commonsenseqa2019,
    title = "{C}ommonsense{QA}: A Question Answering Challenge Targeting Commonsense Knowledge",
    author = "Talmor, Alon  and
      Herzig, Jonathan  and
      Lourie, Nicholas  and
      Berant, Jonathan",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1421",
    doi = "10.18653/v1/N19-1421",
    pages = "4149--4158",
    abstract = "When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56{\%} accuracy, well below human performance, which is 89{\%}.",
}

@article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021},
  url={https://arxiv.org/abs/2110.14168}
}

@misc{gu2023mamba,
      title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, 
      author={Albert Gu and Tri Dao},
      year={2023},
      eprint={2312.00752},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.00752}
}

@inproceedings{
    gu2021combining,
    title={Combining Recurrent, Convolutional, and Continuous-time Models with Linear State Space Layers},
    author={Albert Gu and Isys Johnson and Karan Goel and Khaled Kamal Saab and Tri Dao and Atri Rudra and Christopher Re},
    booktitle={Advances in Neural Information Processing Systems},
    editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
    year={2021},
    url={https://openreview.net/forum?id=yWd42CWN3c}
}

@inproceedings{
    gu2022efficiently,
    title={Efficiently Modeling Long Sequences with Structured State Spaces},
    author={Albert Gu and Karan Goel and Christopher Re},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=uYLFoz1vlAC}
}

@inproceedings{
    mu2023learning,
    title={Learning to Compress Prompts with Gist Tokens},
    author={Jesse Mu and Xiang Lisa Li and Noah Goodman},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://openreview.net/forum?id=2DtxPCL3T5}
}

@inproceedings{
    tay2021long,
    title={Long Range Arena : A Benchmark for Efficient Transformers },
    author={Yi Tay and Mostafa Dehghani and Samira Abnar and Yikang Shen and Dara Bahri and Philip Pham and Jinfeng Rao and Liu Yang and Sebastian Ruder and Donald Metzler},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=qVyeW-grC2k}
}

@misc{lamparth2024human,
      title={Human vs. Machine: Language Models and Wargames}, 
      author={Max Lamparth and Anthony Corso and Jacob Ganz and Oriana Skylar Mastro and Jacquelyn Schneider and Harold Trinkunas},
      year={2024},
      eprint={2403.03407},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2403.03407}
}

@misc{rivera2024escalation,
      title={Escalation Risks from Language Models in Military and Diplomatic Decision-Making}, 
      author={Juan-Pablo Rivera and Gabriel Mukobi and Anka Reuel and Max Lamparth and Chandler Smith and Jacquelyn Schneider},
      year={2024},
      eprint={2401.03408},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2401.03408}
}

@misc{jiang2023mistral,
title={Mistral 7B},
author={Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, Lélio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and El Sayed, William},
year={2023},
eprint={2310.06825},
archivePrefix={arXiv},
primaryClass={cs.CL},
url={https://arxiv.org/abs/2310.06825},
note={Version 1}
}

@misc{hendrycks2022unsolved,
      title={Unsolved Problems in ML Safety}, 
      author={Dan Hendrycks and Nicholas Carlini and John Schulman and Jacob Steinhardt},
      year={2022},
      eprint={2109.13916},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2109.13916}
}

@misc{bommasani2022opportunities,
      title={On the Opportunities and Risks of Foundation Models}, 
      author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and others},
      year={2022},
      eprint={2108.07258},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2108.07258}
}

@misc{li2022explanations,
      title={Explanations from Large Language Models Make Small Reasoners Better}, 
      author={Shiyang Li and Jianshu Chen and Yelong Shen and Zhiyu Chen and Xinlu Zhang and Zekun Li and Hong Wang and Jing Qian and Baolin Peng and Yi Mao and Wenhu Chen and Xifeng Yan},
      year={2022},
      eprint={2210.06726},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.06726}
}

@misc{wang2022rationaleaugmented,
      title={Rationale-Augmented Ensembles in Language Models}, 
      author={Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc Le and Ed Chi and Denny Zhou},
      year={2022},
      eprint={2207.00747},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2207.00747}
}

@inproceedings{
    yao2023react,
    title={ReAct: Synergizing Reasoning and Acting in Language Models},
    author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik R Narasimhan and Yuan Cao},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=WE_vluYUL-X}
}

@misc{
    nye2022show,
    title={Show Your Work: Scratchpads for Intermediate Computation with Language Models},
    author={Maxwell Nye and Anders Johan Andreassen and Guy Gur-Ari and Henryk Michalewski and Jacob Austin and David Bieber and David Dohan and Aitor Lewkowycz and Maarten Bosma and David Luan and Charles Sutton and Augustus Odena},
    year={2022},
    url={https://openreview.net/forum?id=iedYJm92o0a}
}

@inproceedings{
    kojima2022large,
    title={Large Language Models are Zero-Shot Reasoners},
    author={Takeshi Kojima and Shixiang Shane Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},
    booktitle={Advances in Neural Information Processing Systems},
    editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
    year={2022},
    url={https://openreview.net/forum?id=e2TBb5y0yFf}
}

@inproceedings{
    chen2021decision,
    title={Decision Transformer: Reinforcement Learning via Sequence Modeling},
    author={Lili Chen and Kevin Lu and Aravind Rajeswaran and Kimin Lee and Aditya Grover and Michael Laskin and Pieter Abbeel and Aravind Srinivas and Igor Mordatch},
    booktitle={Advances in Neural Information Processing Systems},
    editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
    year={2021},
    url={https://openreview.net/forum?id=a7APmM4B9d}
}

@inproceedings{expert_iteration,
 author = {Anthony, Thomas and Tian, Zheng and Barber, David},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Thinking Fast and Slow with Deep Learning and Tree Search},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d8e1344e27a5b08cdfd5d027d9b8d6de-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{policy_gradient,
author = {Sutton, Richard S. and McAllester, David and Singh, Satinder and Mansour, Yishay},
title = {Policy gradient methods for reinforcement learning with function approximation},
year = {1999},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy.},
booktitle = {Proceedings of the 12th International Conference on Neural Information Processing Systems},
pages = {1057–1063},
numpages = {7},
location = {Denver, CO},
series = {NIPS'99}
}

@misc{Joshi2024,
  title={Personas as a Way to Model Truthfulness in Language Models},
  author={Joshi, Nitish and Rando, Javier and Saparov, Abulhair and Kim, Najoung and He, He},
  year={2024},
  eprint={2310.18168},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  version={5},
  doi={10.48550/arXiv.2310.18168},
  url={https://doi.org/10.48550/arXiv.2310.18168},
  note={arXiv:2310.18168v5 [cs.CL]}
}

@incollection{Hookway2005,
  author       = {Christopher Hookway},
  title        = {William James: Pragmatism: A New Name for Some Old Ways of Thinking},
  booktitle    = {Central Works of Philosophy},
  editor       = {John Shand},
  publisher    = {Acumen Publishing},
  year         = {2005},
  pages        = {54--70}
}

@article{Friston2006,
  author    = {Karl Friston and James Kilner and Lee Harrison},
  title     = {A Free Energy Principle for the Brain},
  journal   = {Journal of Physiology-Paris},
  year      = {2006},
  volume    = {100},
  number    = {1-3},
  pages     = {70-87},
  doi       = {10.1016/j.jphysparis.2006.10.001},
  url       = {https://www.sciencedirect.com/science/article/pii/S092842570600060X}
}

@misc{Tian2023,
  title={Fine-tuning Language Models for Factuality},
  author={Tian, Katherine and Mitchell, Eric and Yao, Huaxiu and Manning, Christopher D. and Finn, Chelsea},
  year={2023},
  eprint={2311.08401},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  doi={10.48550/arXiv.2311.08401},
  url={https://doi.org/10.48550/arXiv.2311.08401},
  note={arXiv:2311.08401 [cs.CL]}
}

@inproceedings{yang-etal-2017-reference,
    title = "Reference-Aware Language Models",
    author = "Yang, Zichao  and
      Blunsom, Phil  and
      Dyer, Chris  and
      Ling, Wang",
    editor = "Palmer, Martha  and
      Hwa, Rebecca  and
      Riedel, Sebastian",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1197",
    doi = "10.18653/v1/D17-1197",
    pages = "1850--1859",
    abstract = "We propose a general class of language models that treat reference as discrete stochastic latent variables. This decision allows for the creation of entity mentions by accessing external databases of referents (required by, e.g., dialogue generation) or past internal state (required to explicitly model coreferentiality). Beyond simple copying, our coreference model can additionally refer to a referent using varied mention forms (e.g., a reference to {``}Jane{''} can be realized as {``}she{''}), a characteristic feature of reference in natural languages. Experiments on three representative applications show our model variants outperform models based on deterministic attention and standard language modeling baselines.",
}

@article{Silver2016,
  author = {D. Silver and A. Huang and C. Maddison and others},
  title = {Mastering the game of Go with deep neural networks and tree search},
  journal = {Nature},
  year = {2016},
  volume = {529},
  pages = {484--489}
}

@inproceedings{ranaldi_freitas_2024_aligning,
    title = "Aligning Large and Small Language Models via Chain-of-Thought Reasoning",
    author = "Leonardo Ranaldi and
      Andre Freitas",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-long.109",
    pages = "1812--1827",
    abstract = "Chain-of-Thought (CoT) prompting empowersthe reasoning abilities of Large Language Models (LLMs), eliciting them to solve complexreasoning tasks in a step-wise manner. However, these capabilities appear only in models with billions of parameters, which represent an entry barrier for many users who are constrained to operate on a smaller model scale, i.e., Small Language Models (SLMs). Although many companies are releasing LLMs of the same family with fewer parameters, these models tend not to preserve all the reasoning capabilities of the original models, including CoT reasoning.In this paper, we propose a method for aligning and transferring reasoning abilities between larger to smaller Language Models. By using an Instruction-tuning-CoT method, that is, an Instruction-tuning designed around CoT-Demonstrations, we enable the SLMs to generate multi-step controlled reasoned answers when they are elicited with the CoT mechanism. Hence, we instruct a smaller Language Model using outputs generated by more robust models belonging to the same family or not, evaluating the impact across different types of models. Results obtained on question-answering and mathematical reasoning benchmarks show that LMs instructed via the Instruction-tuning CoT method produced by LLMs outperform baselines within both in-domain and out-domain scenarios.",
}

@inproceedings{
jang2017categorical,
title={Categorical Reparameterization with Gumbel-Softmax},
author={Eric Jang and Shixiang Gu and Ben Poole},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=rkE3y85ee}
}

@misc{niu2023polynomial,
      title={Polynomial Functors: A Mathematical Theory of Interaction}, 
      author={Nelson Niu and David I. Spivak},
      year={2023},
      eprint={2312.00990},
      archivePrefix={arXiv},
      primaryClass={math.CT},
      url={https://arxiv.org/abs/2312.00990}
}

@inproceedings{
gurnee2024language,
title={Language Models Represent Space and Time},
author={Wes Gurnee and Max Tegmark},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=jE8xbmvFin}
}

@inproceedings{
burns2024discovering,
title={Discovering Latent Knowledge in Language Models Without Supervision},
author={Collin Burns and Haotian Ye and Dan Klein and Jacob Steinhardt},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=ETKGuby0hcs}
}

@inproceedings{
nanda2023progress,
title={Progress measures for grokking via mechanistic interpretability},
author={Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=9XFSbDPmdW}
}

@article{Grabb2024.04.07.24305462,
	author = {Declan Grabb and Max Lamparth and Nina Vasan},
	title = {Risks from Language Models for Automated Mental Healthcare: Ethics and Structure for Implementation},
	elocation-id = {2024.04.07.24305462},
	year = {2024},
	doi = {10.1101/2024.04.07.24305462},
	publisher = {Cold Spring Harbor Laboratory Press},
	abstract = {Amidst the growing interest in developing task-autonomous AI for automated mental health care, this paper addresses the ethical and practical challenges associated with the issue and proposes a structured framework that delineates levels of autonomy, outlines ethical requirements, and defines beneficial default behaviors for AI agents in the context of mental health support. We also evaluate ten state-of-the-art language models using 16 mental health-related questions designed to reflect various mental health conditions, such as psychosis, mania, depression, suicidal thoughts, and homicidal tendencies. The question design and response evaluations were conducted by mental health clinicians (M.D.s). We find that existing language models are insufficient to match the standard provided by human professionals who can navigate nuances and appreciate context. This is due to a range of issues, including overly cautious or sycophantic responses and the absence of necessary safeguards. Alarmingly, we find that most of the tested models could cause harm if accessed in mental health emergencies, failing to protect users and potentially exacerbating existing symptoms. We explore solutions to enhance the safety of current models. Before the release of increasingly task-autonomous AI systems in mental health, it is crucial to ensure that these models can reliably detect and manage symptoms of common psychiatric disorders to prevent harm to users. This involves aligning with the ethical framework and default behaviors outlined in our study. We contend that model developers are responsible for refining their systems per these guidelines to safeguard against the risks posed by current AI technologies to user mental health and safety.Trigger warning Contains and discusses examples of sensitive mental health topics, including suicide and self-harm.Competing Interest StatementThe authors have declared no competing interest.Funding StatementThis study did not receive any funding Author DeclarationsI confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.YesI confirm that all necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived, and that any patient/participant/sample identifiers included were not known to anyone (e.g., hospital staff, patients or participants themselves) outside the research group so cannot be used to identify individuals.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).YesI have followed all appropriate research reporting guidelines, such as any relevant EQUATOR Network research reporting checklist(s) and other pertinent material, if applicable.YesAll data produced in the present study are available upon reasonable request to the authors and our code is available at https://github.com/maxlampe/taimh_eval. https://github.com/maxlampe/taimh_eval},
	URL = {https://www.medrxiv.org/content/early/2024/04/08/2024.04.07.24305462},
	eprint = {https://www.medrxiv.org/content/early/2024/04/08/2024.04.07.24305462.full.pdf},
	journal = {medRxiv}
}
@misc{lamparth2023analyzing,
      title={Analyzing And Editing Inner Mechanisms Of Backdoored Language Models}, 
      author={Max Lamparth and Anka Reuel},
      year={2023},
      eprint={2302.12461},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url = {https://arxiv.org/abs/2302.12461},
}

@misc{biehl2022interpreting,
      title={Interpreting systems as solving POMDPs: a step towards a formal understanding of agency}, 
      author={Martin Biehl and Nathaniel Virgo},
      year={2022},
      eprint={2209.01619},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url = {https://arxiv.org/abs/2209.01619},
}





@inproceedings{meng2022locating,
title={Locating and Editing Factual Associations in {GPT}},
author={Kevin Meng and David Bau and Alex J Andonian and Yonatan Belinkov},
booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
year={2022}
}
%editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
%url={https://openreview.net/forum?id=-h6WAS6eE4}

@article{geva2022transformer,
  title={Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space},
  author={Geva, Mor and Caciularu, Avi and Wang, Kevin Ro and Goldberg, Yoav},
  journal={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={30--45},
  year={2022}
}

@inproceedings{geiger2022inducing,
  title={Inducing causal structure for interpretable neural networks},
  author={Geiger, Atticus and Wu, Zhengxuan and Lu, Hanson and Rozner, Josh and Kreiss, Elisa and Icard, Thomas and Goodman, Noah and Potts, Christopher},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={7324--7338},
  year={2022},
  organization={PMLR}
}


@inproceedings{
    raukur2022toward,
    title={SoK: Toward Transparent {AI}: A Survey on Interpreting the Inner Structures of Deep Neural Networks},
    author={Stephen Casper and Tilman Rauker and Anson Ho and Dylan Hadfield-Menell},
    booktitle={First IEEE Conference on Secure and Trustworthy Machine Learning},
    year={2023},
}
%url={https://openreview.net/forum?id=8C5zt-0Utdn}

%% Currently under review by ICLR 2023: https://openreview.net/forum?id=NpsVSN6o4ul
url = {https://openreview.net/forum?id=NpsVSN6o4ul},
@inproceedings{wang2022interpretability,
	title = {Interpretability in the Wild: a Circuit for Indirect Object Identification in {GPT}-2 Small},
	shorttitle = {Interpretability in the Wild},
	eventtitle = {The Eleventh International Conference on Learning Representations},
    booktitle = {The Eleventh International Conference on Learning Representations},
	author = {Wang, Kevin Ro and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
	urldate = {2023-10-07},
	date = {2022-09-29},
	langid = {english},
	file = {Full Text PDF:/Users/max/Zotero/storage/7J8J57JU/Wang et al. - 2022 - Interpretability in the Wild a Circuit for Indire.pdf:application/pdf},
    year = {2022}
}

@techreport{anthropic2024claude3,
author = {Anthropic},
title = {The Claude 3 Model Family: Opus, Sonnet, Haiku},
institution = {Anthropic},
year = {2024},
month = mar,
url = {https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf}
}

@misc{dubey2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and others},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{wikipediadump,
  author = {Wikimedia Foundation},
  title = {Wikipedia},
  year = {2024},
  url = {https://dumps.wikimedia.org}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@misc{abdin2024phi3technicalreporthighly,
      title={Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone}, 
      author={Marah Abdin and Jyoti Aneja and Hany Awadalla and Ahmed Awadallah and Ammar Ahmad Awan and Nguyen Bach and Amit Bahree and Arash Bakhtiari and Jianmin Bao and others},
      year={2024},
      eprint={2404.14219},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.14219}, 
}

@misc{deepseekai2025,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and others},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}

@misc{christiano2023deepreinforcementlearninghuman,
      title={Deep reinforcement learning from human preferences}, 
      author={Paul Christiano and Jan Leike and Tom B. Brown and Miljan Martic and Shane Legg and Dario Amodei},
      year={2023},
      eprint={1706.03741},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1706.03741}
} 

@inproceedings{zhou2023understanding,
  title={Understanding Chain-of-Thought in LLMs through Information Theory},
  author={Zhou, Dani and Zhou, Enyu and Han, Kevin and Kambadur, Prashant},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}

@article{DBLP:journals/corr/ChungKDGCB15,
  author       = {Junyoung Chung and
                  Kyle Kastner and
                  Laurent Dinh and
                  Kratarth Goel and
                  Aaron C. Courville and
                  Yoshua Bengio},
  title        = {A Recurrent Latent Variable Model for Sequential Data},
  journal      = {CoRR},
  volume       = {abs/1506.02216},
  year         = {2015},
  url          = {http://arxiv.org/abs/1506.02216},
  eprinttype    = {arXiv},
  eprint       = {1506.02216},
  timestamp    = {Mon, 13 Aug 2018 16:46:52 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/ChungKDGCB15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{gu2022efficientlymodelinglongsequences,
      title={Efficiently Modeling Long Sequences with Structured State Spaces}, 
      author={Albert Gu and Karan Goel and Christopher Ré},
      year={2022},
      eprint={2111.00396},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2111.00396}, 
}

@inproceedings{
gu2024mamba,
title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
author={Albert Gu and Tri Dao},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=tEYskw1VY2}
}

@article{rumelhart1986learning,
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  title = {Learning representations by back-propagating errors},
  journal = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  year = {1986},
  doi = {10.1038/323533a0},
  url = {https://doi.org/10.1038/323533a0}
}

@misc{krishnan2015deepkalmanfilters,
      title={Deep Kalman Filters}, 
      author={Rahul G. Krishnan and Uri Shalit and David Sontag},
      year={2015},
      eprint={1511.05121},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1511.05121}, 
}

@misc{karl2017deepvariationalbayesfilters,
      title={Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data}, 
      author={Maximilian Karl and Maximilian Soelch and Justin Bayer and Patrick van der Smagt},
      year={2017},
      eprint={1605.06432},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1605.06432}, 
}
