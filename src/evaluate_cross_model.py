import os
import json
import argparse
import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import savgol_filter
from tqdm import tqdm
from train import (
    find_latest_result,
    calculate_answer_log_probs,
    print_debug_info,
    load_model,
    construct_prompts,
)


def run_cross_model_evaluation(log_files, stride=1, debug_freq=100):
    """
    Evaluate log files using the opposite model as evaluator
    (Mistral for Llama logs and vice versa).

    Args:
        log_files (list): List of log file paths to evaluate
        stride (int): Process every nth entry
        debug_freq (int): How often to print debug info (in terms of processed entries)

    Returns:
        dict: Cross-model evaluation results
    """
    results = {
        "files": log_files,
        "evaluations": [],
        "generator_model": None,  # Will store the model type that generated the log
        "evaluator_model": None,  # Will store the model type used for evaluation
    }

    for file in log_files:
        file_results = []
        with open(file, "r") as f:
            # First line contains hyperparameters
            lines = [json.loads(line) for line in f]
            hyperparameters = lines[0]

            # Store model types
            results["generator_model"] = hyperparameters["model_type"]
            results["evaluator_model"] = (
                "mistral" if hyperparameters["model_type"] == "llama" else "llama"
            )

            print(
                f"Log generated by {results['generator_model']}, evaluating with {results['evaluator_model']}"
            )

            # Load evaluation model
            model, frozen_model, tokenizer, device = load_model(
                model_type=results["evaluator_model"]
            )

            # Create evaluation hyperparameters with correct model type
            eval_hyperparameters = hyperparameters.copy()
            eval_hyperparameters["model_type"] = results["evaluator_model"]

            # Filter entries by stride and create progress bar
            entries = [entry for entry in lines[1:] if "Example" in entry][::stride]
            pbar = tqdm(entries, desc="Processing examples")

            # Process each example
            for i, entry in enumerate(pbar):
                example = entry["Example"]
                question = example["Question"]
                actor_reasoning = example["Actor Reasoning"]
                critic_reasoning = example["Critic Reasoning"]
                answer = example["Answer"]

                # Calculate log probabilities for both reasonings using eval model's hyperparameters
                actor_log_probs, _ = calculate_answer_log_probs(
                    frozen_model,
                    tokenizer,
                    device,
                    [question],
                    [actor_reasoning],
                    [answer],
                    eval_hyperparameters,
                )

                critic_log_probs, _ = calculate_answer_log_probs(
                    frozen_model,
                    tokenizer,
                    device,
                    [question],
                    [critic_reasoning],
                    [answer],
                    eval_hyperparameters,
                )

                # Print debug info periodically
                if i % debug_freq == 0:
                    print("\nDebug Info for Actor Reasoning:")
                    print_debug_info(
                        hyperparameters["task_type"],
                        question,
                        actor_reasoning,
                        answer,
                        actor_log_probs.mean().item(),
                    )
                    print("\nDebug Info for Critic Reasoning:")
                    print_debug_info(
                        hyperparameters["task_type"],
                        question,
                        critic_reasoning,
                        answer,
                        critic_log_probs.mean().item(),
                    )

                # Store results including original normalized reward
                result = {
                    "Batch Index": entry["Batch Index"],
                    "Avg Log Probs": {
                        "Actor": actor_log_probs.mean().item(),
                        "Critic": critic_log_probs.mean().item(),
                    },
                    "Original Reward": entry["Training Metrics"]["Normalized Reward"],
                    "Example": example,
                    "Metrics": entry.get("Training Metrics", {}),
                }
                file_results.append(result)

                # Update progress bar description with current scores
                pbar.set_description(
                    f"Actor: {actor_log_probs.mean().item():.3f}, "
                    f"Critic: {critic_log_probs.mean().item():.3f}"
                )

        results["evaluations"].append(file_results)

    return results


def plot_cross_model_comparison(results, log_file, window_size=40):
    """
    Plot Original Model's Normalized Reward vs Cross-Model Actor-Critic Difference.
    The Actor-Critic difference helps normalize for question difficulty.
    """
    all_data = results["evaluations"]
    min_length = min(len(data) for data in all_data)

    # Initialize averaged data
    averaged_data = {"Actor-Critic Difference": [], "Original Reward": []}

    # Calculate averages across all datasets
    for i in range(min_length):
        # Calculate Actor-Critic differences
        differences = [
            data[i]["Avg Log Probs"]["Actor"] - data[i]["Avg Log Probs"]["Critic"]
            for data in all_data
        ]
        original_rewards = [data[i]["Original Reward"] for data in all_data]

        averaged_data["Actor-Critic Difference"].append(np.mean(differences))
        averaged_data["Original Reward"].append(np.mean(original_rewards))

    plt.figure(figsize=(12, 6))
    colors = ["#e41a1c", "#4daf4a"]  # Red for Difference, Green for Original

    for i, (metric, values) in enumerate(averaged_data.items()):
        if len(values) > window_size:
            smoothed_values = savgol_filter(values, window_size, 3)
            half_window = window_size // 2
            x_values = range(half_window, len(smoothed_values) - half_window)
            y_values = smoothed_values[half_window:-half_window]
            plt.plot(x_values, y_values, label=metric, color=colors[i], linewidth=2)
        else:
            plt.plot(values, label=metric, color=colors[i], linewidth=2)

    plt.xlabel("Sample", fontsize=16)
    plt.ylabel("Score", fontsize=16)
    plt.title(
        f"{results['generator_model'].title()} Generated, {results['evaluator_model'].title()} Evaluated\n"
        f"Original Reward vs Actor-Critic Difference (Smoothing: {window_size})",
        fontsize=16,
    )
    plt.legend(fontsize=12, loc="lower right")
    plt.grid(True, linestyle="--", alpha=0.7)
    plt.tick_params(axis="both", which="major", labelsize=14)
    plt.tight_layout()

    output_file = os.path.join(os.path.dirname(log_file), "cross_model_evaluation.png")
    plt.savefig(output_file, dpi=300, bbox_inches="tight")
    print(f"Plot saved to {output_file}")


def save_evaluation_results(results, log_file):
    """Save evaluation results to a new jsonl file."""
    output_file = os.path.join(
        os.path.dirname(log_file), "cross_model_evaluation_results.jsonl"
    )

    with open(output_file, "w") as f:
        # Write metadata as first line
        json.dump(
            {
                "files": results["files"],
                "generator_model": results["generator_model"],
                "evaluator_model": results["evaluator_model"],
            },
            f,
        )
        f.write("\n")

        # Write evaluation results
        for eval_set in results["evaluations"]:
            for entry in eval_set:
                json.dump(entry, f)
                f.write("\n")

    print(f"Evaluation results saved to {output_file}")
    return output_file


def load_evaluation_results(results_file):
    """Load evaluation results from jsonl file."""
    results = {"evaluations": [[]]}  # Initialize with single evaluation set

    with open(results_file, "r") as f:
        # First line contains metadata
        metadata = json.loads(f.readline())
        results["files"] = metadata["files"]
        results["generator_model"] = metadata["generator_model"]
        results["evaluator_model"] = metadata["evaluator_model"]

        # Read evaluation results
        for line in f:
            results["evaluations"][0].append(json.loads(line))

    return results


def main():
    parser = argparse.ArgumentParser(description="Cross-Model Evaluation Tool")
    parser.add_argument("--log_file", help="Log file to evaluate")
    parser.add_argument(
        "--window_size", type=int, default=40, help="Smoothing window size"
    )
    parser.add_argument("--stride", type=int, default=1, help="Process every nth entry")
    parser.add_argument(
        "--debug_freq",
        type=int,
        default=100,
        help="Print debug info every n processed entries",
    )
    parser.add_argument(
        "--process_only", action="store_true", help="Only process data without plotting"
    )
    parser.add_argument(
        "--plot_only",
        action="store_true",
        help="Only generate plots from saved results",
    )
    args = parser.parse_args()

    if args.log_file:
        log_file = args.log_file
    else:
        log_file = find_latest_result(return_log=True)

    if not log_file:
        print("No log file found.")
        return

    print(f"Using log file: {log_file}")

    # Determine evaluation results file path
    eval_results_file = os.path.join(
        os.path.dirname(log_file), "cross_model_evaluation_results.jsonl"
    )

    # Process data if needed
    if not args.plot_only:
        print(f"Processing every {args.stride}th entry")
        results = run_cross_model_evaluation(
            [log_file], stride=args.stride, debug_freq=args.debug_freq
        )
        save_evaluation_results(results, log_file)

    # Plot if needed
    if not args.process_only:
        try:
            results = load_evaluation_results(eval_results_file)
            plot_cross_model_comparison(results, log_file, window_size=args.window_size)
        except FileNotFoundError:
            print(
                f"No saved results found at {eval_results_file}. Run without --plot_only first."
            )


if __name__ == "__main__":
    main()
