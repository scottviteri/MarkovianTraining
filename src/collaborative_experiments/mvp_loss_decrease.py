"""
for llama use
```
torchrun --nproc_per_node 1 src/collaborative_experiments/mvp_loss_decrease.py
```
# Experiment outline:
## Goal:
    1. Measure a decrease in the loss of a model on a particular high quality dataset 
        when a helpful message is prepended to the prompt.
## Hypothesis:
    1. There are strings you can pre-pend to a message that will decrease the loss of the model
    2. Downstream hypothesis if this works: We can see if a LM is able to figure out how to provide such helpful and honest messages.
## Method:
    1. Train a model on a dataset
    2. Measure the loss of the model on each batch of the dataset with and without a prompt
## Evaluation and Experiments:
    1. Look for potential complications by plotting per token loss as a function of token position.
    2. Evaluate null hypothesis: just a random sentence prepended of a fixed length.
    3. Evaluate hypotheses: a) The first sentence from the batch. b) The first sentence wrapped in a message explaining this is a
        helpful message. c) A model generated summary of the sentences. - Helpful message generated by gpt4 as upper bound on competence
"""

import os
import sys
import time
from pathlib import Path
import json

import torch
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from tqdm import tqdm
import accelerate
import plotly.express as px
import pandas as pd
import numpy as np
import openai

from concurrent.futures import ThreadPoolExecutor
from tenacity import (
    retry,
    stop_after_attempt,
    wait_random_exponential,
)  # for exponential backoff

from collaborative_experiments.constants import MAX_CONTEXT_LENGTH, MSG_CONTEXT_LENGTH


class ExperimentConfig:
    def __init__(self, msg_fn, expected_length, name):
        self.msg_fn = msg_fn
        self.expected_length = expected_length  # measured in number of tokens
        self.name = name


def get_device():
    """
    Get's either cuda, cpu, or mps, using accelerate
    """
    accelerator = accelerate.Accelerator()
    device = accelerator.device
    return device


def tile_a_tensor(reshaped_tensor):
    reshaped_tensor.fill_(50)  # shape (2, data_context_length)
    reshaped_tensor = reshaped_tensor[0:2]
    for i in range(reshaped_tensor.shape[1] // 3):
        reshaped_tensor[0, i * 3 + 1] += 1
        reshaped_tensor[1, i * 3 + 1] += 1
        reshaped_tensor[0, i * 3 + 2] += 2
        reshaped_tensor[1, i * 3 + 2] += 2
    return reshaped_tensor


def load_and_format_dataset(
    textbook_1_path, causal_lm_tokenizer, debug=False, reduced_data=0
):
    data_context_length = MAX_CONTEXT_LENGTH - MSG_CONTEXT_LENGTH
    dataset = load_dataset("text", data_files=textbook_1_path)
    print(dataset)

    # collapse dataset into one big string
    dataset_1 = dataset["train"]
    text = "\n".join(dataset_1["text"])

    # tokenize dataset
    dataset_1_tokenized = causal_lm_tokenizer(text, return_tensors="pt")

    # convert from shape (1, num_tokens) to (num_tokens/data_context_length, data_context_length)
    tokens_tensor = dataset_1_tokenized["input_ids"].squeeze()
    size = tokens_tensor.shape[0]
    size = (size // data_context_length) * (data_context_length)
    tokens_tensor = tokens_tensor[0:size]
    reshaped_tensor = tokens_tensor.view(-1, data_context_length)
    print(
        reshaped_tensor.shape
    )  # Should print torch.Size([num_tokens/data_context_length, data_context_length])
    # turn all values to be the same 11
    if debug:
        reshaped_tensor = tile_a_tensor(reshaped_tensor)
    elif reduced_data > 0:
        reshaped_tensor = reshaped_tensor[0:reduced_data]

    return reshaped_tensor


def create_helpful_message_1(tokens, tokens_to_grab=MSG_CONTEXT_LENGTH):
    """
    Simply takes a message, grabs the first tokens_to_grab tokens of the data
    and prepends it to the message to make it max context length

    Args:
        tokens (torch.tensor): the tokens to prepend the message to, shape (batch_size, MAX_CONTEXT_LENGTH - MSG_CONTEXT_LENGTH)
        tokens_to_grab (int): the number of tokens to grab from the data as the msg
    Returns
        (torch.tensor): the message with the data prepended
    """
    msg = tokens[:, 0:tokens_to_grab]
    return torch.cat((msg, tokens), dim=1)


def create_helpful_message_2(tokens, tokens_to_grab=MSG_CONTEXT_LENGTH):
    """
    Same as 1 except keeps the shape the same
    """
    tokens_to_grab = (
        int(tokens.shape[1] / 2) if tokens_to_grab is None else tokens_to_grab
    )
    msg = tokens[:, 0:tokens_to_grab]
    return torch.cat((msg, tokens[:, 0:-tokens_to_grab]), dim=1)


@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(10))
def create_openai_helpful_message(tokens, causal_lm_tokenizer, causal_lm, system_prompt=None, user_prompt=None, print_msg=False):
    # Convert tokens to text
    text = causal_lm_tokenizer.decode(tokens[0])
    # Make a chat completion call to GPT-3.5
    if system_prompt is None:
        system_prompt = "You are a language model's assistant, and your job is to make 'prepend text that makes the following text as predictable as possible. Do not be afraid to copy surprising parts of the text verbatim."
    if user_prompt is None:
        user_prompt = "Please generate a prepend string for the following text: "
    user_prompt += text
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "system",
                "content": system_prompt,
            },
            {
                "role": "user",
                "content": user_prompt,
            },
        ],
    )
    # Get the prepend string from the response
    prepend_string = response.choices[0].message["content"]
    # Convert the summary back to tokens
    summary_tokens = causal_lm_tokenizer.encode(prepend_string, return_tensors="pt")
    # Ensure the length of the summary is at most a quarter the length of tokens
    quarter_length = tokens.shape[1] // 4
    if summary_tokens.shape[1] > quarter_length:
        summary_tokens = summary_tokens[:, :quarter_length]
    # Prepend the summary tokens to the original tokens
    new_tokens = torch.cat(
        (summary_tokens, tokens[:, : -summary_tokens.shape[1]]), dim=1
    )
    # Decode the summary tokens for printing
    decoded_main_tokens = causal_lm_tokenizer.decode(
        tokens[:, : -summary_tokens.shape[1]][0]
    )
    if print_msg: print("Prepend string: ", prepend_string, "\nMain string: ", decoded_main_tokens)
    return new_tokens


def train_step(
    batch,
    causal_lm,
    loss_fn,
    device,
    correct_probs_all,
    verbose=False,
    debug=False,
    pytest=False,
):
    # make labels from the batch, one hot encoded of shape (batch_size, seq_len, vocab_size)
    batch = batch.to(device)
    shifted_batch = batch[
        :, 1:
    ]  # we shift because the logits predict one in the future
    if correct_probs_all.shape[0] != shifted_batch.shape[1]:
        raise ValueError(
            f"correct_probs_all and shifted_batch should have the same shape. Shifted_batch_shape {shifted_batch.shape} but got correct_probs_all shape of {correct_probs_all.shape}"
        )
    labels = torch.nn.functional.one_hot(
        shifted_batch, num_classes=causal_lm.config.vocab_size
    ).to(torch.float32)
    outputs_original = causal_lm(
        input_ids=batch.to(device)
    )  # maybe I don't want past_ke_values to be returned? what is that?
    # at this point outputs_original is logits and past_key_values
    logits = outputs_original.logits.detach()  # to("cpu")
    logits_shifted = logits[
        :, :-1, :
    ]  # shift because the last value is a prediction for a label we don't have
    loss = loss_fn(logits_shifted, labels)
    if verbose:
        tqdm.write(f"{loss}")

    # Compute softmax over the last dimension to get probabilities
    probs = F.softmax(logits_shifted, dim=-1)  # shape (batch_size, seq_len, vocab_size)

    # Use gather to pick the probabilities corresponding to the correct token at each position
    correct_probs = (
        probs.gather(-1, shifted_batch.unsqueeze(-1)).squeeze(-1).detach()
    )  # shape (batch_size, seq_len)
    correct_probs_all += correct_probs.mean(dim=0)
    if debug:
        print("batch: ", batch)
        print("correct probs: ", correct_probs)
        # get one sentence
        # print the tokens that it assigns max probability to
        # print the actual sentence
        sentence_tokens = batch[0]
        sentence_probs = probs[0]
        sentence_correct_probs = correct_probs[0]
    if pytest:
        return loss, logits_shifted
    return loss.to("cpu")


def run_experiment(config, dataset_1_loader, causal_lm, loss_fn, device):
    # we subtract one to the size because causal_lm will predict the next token
    # therefore we can only check predictions for the first expected_length - 1 tokens
    correct_probs_all = torch.zeros(config.expected_length - 1).to(device)
    losses = []

    if False and "openai" in config.name:
        all_batches = []
        for batch in tqdm(dataset_1_loader, desc=f"Experiment {config.name}"):
            all_batches.append(batch)
        with ThreadPoolExecutor(max_workers=10) as executor:
            updated_batchs = list(tqdm(executor.map(
                config.msg_fn, all_batches, chunksize=1
            ), total=len(all_batches)))  # this is a generator
        # update data_loader with the new batches
        dataset_1_loader = torch.utils.data.DataLoader(
            updated_batchs, batch_size=1, shuffle=False
        )
        config.msg_fn = lambda x: x
        
    
    for batch in tqdm(dataset_1_loader, desc=f"Experiment {config.name}"):
        batch = config.msg_fn(batch)
        loss = train_step(batch, causal_lm, loss_fn, device, correct_probs_all)
        losses.append(loss)

    return losses, correct_probs_all.to("cpu")


def load_llama_model(
    ckpt_dir: str = "../llama/llama-2-7b",
    tokenizer_path: str = "../llama/tokenizer.model",
    temperature: float = 0.6,
    top_p: float = 0.9,
    max_seq_len: int = 1024,
    max_gen_len: int = 64,
    max_batch_size: int = 8,
    device: str = "mps",
):
    os.environ["RANK"] = "0"
    # generator = Llama.build(
    #     ckpt_dir=ckpt_dir,
    #     tokenizer_path=tokenizer_path,
    #     max_seq_len=max_seq_len,
    #     max_batch_size=max_batch_size,
    # ) # fails for strange reasons
    # return generator.model, generator.tokenizer
    start_time = time.time()
    checkpoints = sorted(Path(ckpt_dir).glob("*.pth"))
    for chkpt_path in checkpoints:
        checkpoint = torch.load(chkpt_path, map_location="cpu")
    with open(Path(ckpt_dir) / "params.json", "r") as f:
        params = json.loads(f.read())

    model_args: ModelArgs = ModelArgs(
        max_seq_len=max_seq_len,
        max_batch_size=max_batch_size,
        **params,
    )
    tokenizer = Tokenizer(model_path=tokenizer_path)
    if device == "cuda":
        torch.set_default_tensor_type(torch.cuda.HalfTensor)
    elif device == "mps":
        torch.set_default_tensor_type(torch.HalfTensor)
    else:
        torch.set_default_tensor_type(torch.BFloat16Tensor)
    model = Transformer(model_args)
    model.load_state_dict(checkpoint, strict=False)
    print(f"Loaded in {time.time() - start_time:.2f} seconds")
    return model, tokenizer


def main(
    save_dir="results_debug",
    debug=False,
    BATCH_SIZE=1,
    model_name="distilgpt2",
    reduced_data=10,
):
    device = get_device()
    if model_name == "llama":
        from llama import Llama
        from llama.model import ModelArgs, Transformer
        from llama.tokenizer import Tokenizer

        causal_lm, causal_lm_tokenizer = load_llama_model(device=device)
    elif model_name == "gpt-neo":
        causal_lm = AutoModelForCausalLM.from_pretrained("EleutherAI/gpt-neo-2.7B")
        causal_lm_tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neo-2.7B")
    else:
        causal_lm = AutoModelForCausalLM.from_pretrained("distilgpt2")
        causal_lm_tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
    print("Loaded causal LM")
    print(causal_lm)
    causal_lm = causal_lm.to(device)
    print("Loaded causal LM to device")

    # load dataset
    # https://www.gutenberg.org/ebooks/71431
    textbook_1_path = "data/st_patrick_biography.txt"
    reshaped_tensor = load_and_format_dataset(
        textbook_1_path, causal_lm_tokenizer, debug=debug, reduced_data=reduced_data
    )
    ## make a pytorch data loader for the dataset
    dataset_1_loader = torch.utils.data.DataLoader(
        reshaped_tensor, batch_size=BATCH_SIZE, shuffle=True
    )

    loss_fn = torch.nn.CrossEntropyLoss()

    experiments = []
    experiments.append(
        ExperimentConfig(
            lambda x: create_openai_helpful_message(x, causal_lm_tokenizer, causal_lm, user_prompt=user_prompt),
            reshaped_tensor.shape[1],
            "openai_helpful_message",
        )
    )
    experiments.append(
        ExperimentConfig(lambda x: x, reshaped_tensor.shape[1], "original")
    )
    # system_prompt = ""
    user_prompt = "Please generate a succinct summary of the following text in about 64 words: "
    experiments.append(
        ExperimentConfig(
            create_helpful_message_2, reshaped_tensor.shape[1], "helpful_message_2"
        )
    )

    losses_dict = {}
    correct_probs_all_dict = {}
    for experiment in tqdm(experiments, desc="Experiment"):
        losses, correct_probs_all = run_experiment(
            experiment, dataset_1_loader, causal_lm, loss_fn, device
        )
        losses_dict[experiment.name] = losses
        correct_probs_all_dict[experiment.name] = correct_probs_all.clone().numpy() / (
            len(dataset_1_loader) * BATCH_SIZE
        )

    for exp_name, losses in losses_dict.items():
        print(f"experiment {exp_name} had avg loss of {np.mean(losses)}")

    if not os.path.exists(f"{save_dir}"):
        os.makedirs(f"{save_dir}")
    save_dir = os.path.join(save_dir, f"{model_name}")
    if not os.path.exists(f"{save_dir}"):
        os.makedirs(f"{save_dir}")
    # plot the losses on the same graph

    # Convert tensors to numpy arrays
    for key in losses_dict:
        losses_dict[key] = [loss.item() for loss in losses_dict[key]]

    df = pd.DataFrame(losses_dict)
    df["batch_index"] = df.index
    df = df.melt(id_vars=["batch_index"], value_vars=list(losses_dict.keys()))
    fig = px.line(df, x="batch_index", y="value", color="variable")
    fig.update_layout(title=f"Losses, batch_size {BATCH_SIZE}")
    fig.show()
    fig.write_html(f"{save_dir}/losses.html")

    # plot the per token posisions on the same graph
    # normalize the lengths of the different experiments by padding to the max one with zeros
    max_len = max([len(x) for x in correct_probs_all_dict.values()])
    for exp_name, correct_probs_all in correct_probs_all_dict.items():
        if len(correct_probs_all) < max_len:
            correct_probs_all_dict[exp_name] = np.pad(
                correct_probs_all,
                (0, max_len - len(correct_probs_all)),
                "constant",
                constant_values=0,
            )
    df = pd.DataFrame(correct_probs_all_dict)
    df["position"] = df.index
    df = df.melt(id_vars=["position"], value_vars=list(correct_probs_all_dict.keys()))
    fig = px.line(df, x="position", y="value", color="variable")
    fig.update_layout(title="Probability of correct token at each position")
    fig.show()
    fig.write_html(f"{save_dir}/probability_of_correct_token_at_each_position.html")


if __name__ == "__main__":
    import fire

    fire.Fire(main)
