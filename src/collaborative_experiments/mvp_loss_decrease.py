"""
for llama use
```
torchrun --nproc_per_node 1 src/collaborative_experiments/mvp_loss_decrease.py
```
# Experiment outline:
## Goal:
    1. Measure a decrease in the loss of a model on a particular high quality dataset 
        when a helpful message is prepended to the prompt.
## Hypothesis:
    1. There are strings you can pre-pend to a message that will decrease the loss of the model
    2. Downstream hypothesis if this works: We can see if a LM is able to figure out how to provide such helpful and honest messages.
## Method:
    1. Train a model on a dataset
    2. Measure the loss of the model on each batch of the dataset with and without a prompt
## Evaluation and Experiments:
    1. Look for potential complications by plotting per token loss as a function of token position.
    2. Evaluate null hypothesis: just a random sentence prepended of a fixed length.
    3. Evaluate hypotheses: a) The first sentence from the batch. b) The first sentence wrapped in a message explaining this is a
        helpful message. c) A model generated summary of the sentences. - Helpful message generated by gpt4 as upper bound on competence
"""

import os
import torch
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.models.gpt2.modeling_gpt2 import GPT2LMHeadModel
from peft.peft_model import PeftModelForCausalLM
from tqdm import tqdm
import plotly.express as px
import pandas as pd
import numpy as np

# import openai
import wandb

from torchtyping import TensorType, patch_typeguard
from typeguard import typechecked
from torch.utils.data import DataLoader
from transformers import PreTrainedTokenizerFast
from transformers.models.auto.modeling_auto import AutoModelForCausalLM
from typing import *

# from jaxtyping import Array, Int, Float

from concurrent.futures import ThreadPoolExecutor
from tenacity import (
    retry,
    stop_after_attempt,
    wait_random_exponential,
    retry_if_exception_type,
)

from collaborative_experiments.constants import (
    DEFAULT_MAX_CONTEXT_LENGTH,
    DEFAULT_MSG_CONTEXT_LENGTH,
)
from collaborative_experiments.utils import (
    get_device,
    load_and_format_dataset,
    load_llama_model,
)
from collaborative_experiments.mocking import mockCausalGPT2

import logging

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

LOGGING_DICT_WANDB = {}


# class OpenAIException(Exception):
#     """
#     Custom exception to make sure we only retry due to errors from OpenAI
#     and not on other errors.
#     """
#
#     def __init__(self, Exception):
#         self.Exception = Exception
#         self.message = str(Exception)


class ExperimentConfig:
    def __init__(self, msg_fn, expected_length, name):
        self.msg_fn = msg_fn
        self.expected_length = expected_length  # measured in number of tokens
        self.name = name


@typechecked
def create_helpful_message_1(
    tokens,
    tokens_to_grab: int = DEFAULT_MSG_CONTEXT_LENGTH,
):
    """
    Returns the first tokens_to_grab tokens of the input tokens
    """
    msg = tokens[:, :tokens_to_grab]
    return msg


@typechecked
def create_model_helpful_message(
    uncompressed_tokens: TensorType,
    causal_lm_tokenizer: PreTrainedTokenizerFast,
    causal_lm: Union[AutoModelForCausalLM, PeftModelForCausalLM, GPT2LMHeadModel],
    custom_user_prompt: Optional[str] = None,
    max_helpful_message_length: int = DEFAULT_MSG_CONTEXT_LENGTH,
):  # -> TensorType:
    """
    Creates a helpful message using the causal language model

    Args:
        uncompressed_tokens (TensorType): The input tokens
        causal_lm_tokenizer (PreTrainedTokenizerFast): The tokenizer used by the causal language model
        causal_lm (AutoModelForCausalLM): The causal language model
        custom_user_prompt (Optional[str]): Custom user prompt. If None, a default prompt is used
        max_helpful_message_length (int): The maximum length of the helpful message

    Returns:
        TensorType: The helpful message
    """
    text = causal_lm_tokenizer.decode(uncompressed_tokens[0])
    if custom_user_prompt is None:
        custom_user_prompt = "You are a language model's assistant, and your job is to prepend text that makes the following text as predictable as possible. Do not be afraid to copy surprising parts of the text verbatim. <Begin Text-to-Summarize> "
    custom_user_prompt += text + "</End Text-To-Summarize> <Begin Summarization> "
    converted_tokens = (
        causal_lm_tokenizer.encode(custom_user_prompt, return_tensors="pt")
        .to(causal_lm.device)
        .to(torch.int32)
    )
    # Ensure the number of tokens in the message does not exceed the model's maximum position embeddings
    # FIXME: GPTNeo config uses max_position_embeddings instead of n_positions
    assert (
        converted_tokens.shape[1] + max_helpful_message_length
        <= causal_lm.config.n_positions
    ), "The total number of tokens exceeds the model's maximum position embeddings"

    seq_len = converted_tokens.shape[1]
    # only return new tokens
    # causal_lm = causal_lm.to("cpu")
    helpful_message = causal_lm.generate(
        # input_ids=converted_tokens.to("cpu"),
        input_ids=converted_tokens,
        max_new_tokens=max_helpful_message_length,
        do_sample=True,
        top_p=0.90,
        temperature=0.7,
        num_return_sequences=1,
        pad_token_id=causal_lm_tokenizer.pad_token_id,
    )[:, seq_len:]
    assert (
        helpful_message.shape[1] <= max_helpful_message_length
    ), "somehow the message is longer than the max length"
    if helpful_message.shape[1] < max_helpful_message_length:
        padding_length = max_helpful_message_length - helpful_message.shape[1]
        helpful_message = pad_msg(
            helpful_message, padding_length, causal_lm_tokenizer.encode("-")[0]
        )
    return helpful_message


@typechecked
def pad_msg(
    msg_tokens: TensorType, pad_length: int, pad_token_id: int = 0
) -> TensorType:
    padding = torch.full((1, pad_length), pad_token_id, device=msg_tokens.device)
    msg_tokens = torch.cat([padding, msg_tokens], dim=1)
    return msg_tokens


# @retry(
#     wait=wait_random_exponential(max=10),
#     stop=stop_after_attempt(2),
#     retry=retry_if_exception_type(OpenAIException),
# )
# @typechecked
# def create_openai_helpful_message(
#     tokens: TensorType,  #: TensorType,
#     causal_lm_tokenizer: PreTrainedTokenizerFast,
#     system_prompt: Optional[str] = None,
#     user_prompt: Optional[str] = None,
#     print_msg: bool = False,
#     msg_context_length: int = DEFAULT_MSG_CONTEXT_LENGTH,
# ) -> TensorType:  # -> TensorType:
#     print("trying to create openai helpful message")
#     # Convert tokens to text
#     text = causal_lm_tokenizer.decode(tokens[0])
#     # Make a chat completion call to GPT-3.5
#     if system_prompt is None:
#         system_prompt = "You are a language model's assistant, and your job is to make 'prepend text that makes the following text as predictable as possible. Do not be afraid to copy surprising parts of the text verbatim."
#     if user_prompt is None:
#         user_prompt = "Please generate a prepend string for the following text: "
#     user_prompt += text
#     try:
#         response = openai.ChatCompletion.create(
#             model="gpt-3.5-turbo",
#             messages=[
#                 {
#                     "role": "system",
#                     "content": system_prompt,
#                 },
#                 {
#                     "role": "user",
#                     "content": user_prompt,
#                 },
#             ],
#         )
#     except Exception as e:
#         raise OpenAIException(e)
#     # Get the prepend string from the response
#     prepend_string = response.choices[0].message["content"]
#     # Convert the summary back to tokens
#     msg_tokens = causal_lm_tokenizer.encode(prepend_string, return_tensors="pt")
#     # Decode the summary tokens for printing
#     decoded_main_tokens = causal_lm_tokenizer.decode(
#         tokens[:, : -msg_tokens.shape[1]][0]
#     )
#     if print_msg:
#         print(
#             "Prepend string: ", prepend_string, "\nMain string: ", decoded_main_tokens
#         )
#     if msg_tokens.shape[1] > msg_context_length:
#         msg_tokens = msg_tokens[:, :msg_context_length]
#     if msg_tokens.shape[1] < msg_context_length:
#         padding_length = msg_context_length - msg_tokens.shape[1]
#         msg_tokens = pad_msg(
#             msg_tokens, padding_length, causal_lm_tokenizer.encode("-")[0]
#         )
#     return msg_tokens
#


def msg_loss(
    original_text: TensorType,
    helpful_msg: TensorType,
    causal_lm: AutoModelForCausalLM,
    loss_fn: Callable,
    device: torch.device,
    requires_grad: bool = False,
) -> Tuple[
    float,
    TensorType,
    TensorType,
    TensorType,
]:
    msg_length: int = helpful_msg.shape[1]
    # Get the logits for original_text
    model_input: TensorType = torch.cat((helpful_msg, original_text), dim=1)
    model_input = model_input.to(device)
    outputs_original = causal_lm(input_ids=model_input)
    logits = outputs_original.logits
    if not requires_grad:
        logits = logits.detach()
    logits_shifted: TensorType = logits[
        :, msg_length:-1, :
    ]  # negative one because prediction shifts things by one
    # should be shape (b, data_context_length - 1, vocab_size)

    # now create one hot labels to get the loss with.
    shifted_model_input: TensorType = model_input[
        :, msg_length + 1 :
    ]  # we shift 1 more because the logits predict one in the future
    labels: TensorType = torch.nn.functional.one_hot(
        shifted_model_input, num_classes=causal_lm.config.vocab_size
    ).to(torch.float32)
    loss: float = loss_fn(logits_shifted[:,], labels)  # only calculate loss on original_text

    return loss, logits_shifted, shifted_model_input, model_input


def train_step(
    batch: Dict[str, TensorType],
    causal_lm: AutoModelForCausalLM,
    loss_fn: Callable,
    device: torch.device,
    verbose: bool = False,
    debug: bool = False,
    pytest: bool = False,
) -> Tuple[TensorType, TensorType, TensorType,]:
    """
    Args:
        batch (dict): a dictionary with a 'msg' key and a 'content' key. The 'msg' key is a tensor of shape (batch_size, msg_context_length)
            and the 'content' key is a tensor of shape (batch_size, data_context_length)
    Returns:
        loss (torch.tensor): a tensor of shape (batch_size, data_context_length - 1)
        correct_probs (torch.tensor): a tensor of shape (batch_size, data_context_length - 1)
        if pytest, returns logits_shifted (torch.tensor): a tensor of shape (batch_size, data_context_length - 1, vocab_size)
    """
    helpful_msg: TensorType = batch[
        "helpful_msg"
    ]  # shape (batch_size, msg_context_length)
    original_text : TensorType = batch[
        "original_text"
    ]  # shape (batch_size, data_context_length)
    loss, logits_shifted, shifted_model_input, model_input = msg_loss(
        original_text, helpful_msg, causal_lm, loss_fn, device
    )

    if verbose:
        tqdm.write(f"{loss}")

    # Compute softmax over the last dimension to get probabilities
    probs = F.softmax(
        logits_shifted, dim=-1
    )  # shape (model_input_size, seq_len, vocab_size)

    # Use gather to pick the probabilities corresponding to the correct token at each position
    correct_probs = (
        probs.gather(-1, shifted_model_input.unsqueeze(-1)).squeeze(-1).detach()
    )  # shape (model_input_size, seq_len)
    if debug:
        print("model_input: ", model_input)
        print("correct probs: ", correct_probs)
        # get one sentence
        # print the tokens that it assigns max probability to
        # print the actual sentence
        sentence_tokens = model_input[0]
        sentence_probs = probs[0]
        sentence_correct_probs = correct_probs[0]
    if pytest:
        return loss, correct_probs, logits_shifted
    return loss.to("cpu"), correct_probs


#
# def batched_create_openai_msgs(
#     dataset_1_loader: DataLoader, config: ExperimentConfig
# ) -> List[TensorType]:
#     all_batches: List[TensorType] = []
#     for batch in tqdm(dataset_1_loader, desc=f"Experiment {config.name}"):
#         all_batches.extend(batch)
#     all_batches_dataloader: DataLoader = torch.utils.data.DataLoader(
#         all_batches, batch_size=1, shuffle=False
#     )
#     with ThreadPoolExecutor(max_workers=10) as executor:
#         messages_batched: List[TensorType] = list(
#             tqdm(
#                 executor.map(config.msg_fn, all_batches_dataloader, chunksize=1),
#                 total=len(all_batches),
#                 desc=f"Reformating exp {config.name} for multi-threading",
#             )
#         )
#     return messages_batched


def run_experiment(
    config: ExperimentConfig,
    dataset_1_loader: DataLoader,
    causal_lm: AutoModelForCausalLM,
    loss_fn: Callable,
    device: torch.device,
    batched_openai: bool = True,
    verbose: bool = False,
) -> Tuple[List[TensorType], TensorType]:
    # we subtract one to the size because causal_lm will predict the next token
    # therefore we can only check predictions for the first expected_length - 1 tokens
    correct_probs_all: TensorType = torch.zeros(config.expected_length - 1).to(device)
    losses: List[TensorType] = []

    if batched_openai and "openai" in config.name:
        pass
        # messages_batched: Iterator[TensorType] = iter(
        #     batched_create_openai_msgs(dataset_1_loader, config)
        # )
        # # function is now an iterator over messages_batched, ignores the batch
        # config.msg_fn = lambda x: messages_batched.__next__()

    if config.name == "original":
        LOGGING_DICT_WANDB[f"{config.name}_original_text"] = []
    else:
        LOGGING_DICT_WANDB[f"{config.name}_helpful_msg_decoded"] = []
    for i, batch in enumerate(tqdm(dataset_1_loader, desc=f"Experiment {config.name}")):
        batch_dict: Dict[str, TensorType] = {}
        batch_dict["helpful_msg"] = config.msg_fn(batch)
        tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(
            "EleutherAI/gpt-neo-2.7B"
        )
        helpful_msg_decoded: str = tokenizer.decode(batch_dict["helpful_msg"][0])
        if verbose:
            tqdm.write("helpful_msg: " + helpful_msg_decoded)
            tqdm.write(f"---end-helpful-msg-{config.name}--")
            tqdm.write("original_text: " + tokenizer.decode(batch[0]))
            tqdm.write(f"---end-original_text--{config.name}--")
        if config.name == "original":
            LOGGING_DICT_WANDB[f"{config.name}_original_text"].append(
                tokenizer.decode(batch[0])
            )
        else:
            LOGGING_DICT_WANDB[f"{config.name}_helpful_msg_decoded"].append(helpful_msg_decoded)

        batch_dict["original_text"] = batch
        loss, correct_probs, _ = train_step(
            batch=batch_dict, causal_lm=causal_lm, loss_fn=loss_fn, device=device
        )
        correct_probs_all += correct_probs.mean(dim=0)
        wandb.log({f"{config.name}_loss": loss.item()}, commit=False)
        wandb.log({f"{config.name}_correct_probs": correct_probs.mean(dim=0)})
        losses.append(loss)

    return losses, correct_probs_all.to("cpu")


def main(
    save_directory: str = "results_debug",
    debug_mode: bool = False,
    batch_size: int = 1,
    model_type: str = "distilgpt2",
    debug_dataset_size: int = 10,
    training_context_length: int = DEFAULT_MAX_CONTEXT_LENGTH,
    message_context_length: int = DEFAULT_MSG_CONTEXT_LENGTH,
    experiment_list: str = "all",
    data_file_location: str = "data/st_patrick_biography.txt",
    batched_openai: bool = True,
    verbose_output: bool = True,
):
    if batch_size != 1:
        raise NotImplementedError(
            "Only implemented for batch size 1, not {}".format(batch_size)
        )
    if "mock" in model_type:
        os.environ["WANDB_MODE"] = "dryrun"
    else:
        os.environ["WANDB_MODE"] = "online"
    wandb.init(
        project="collaborative_training",
        config={
            "run_finished_succesfully": False,
            "model_type": model_type,
            "save_directory": save_directory,
            "experiment_list": experiment_list,
            "debug_dataset_size": debug_dataset_size,
            "training_context_length": training_context_length,
            "message_context_length": message_context_length,
            "batch_size": batch_size,
            "debug_mode": debug_mode,
            "data_file_location": data_file_location,
            "batched_openai": batched_openai,
            "verbose_output": verbose_output,
        },
    )
    device: torch.device = get_device(model_type)
    if model_type == "llama":
        causal_lm, causal_lm_tokenizer = load_llama_model(device=device)
    elif model_type == "gpt-neo":
        causal_lm: AutoModelForCausalLM = AutoModelForCausalLM.from_pretrained(
            "EleutherAI/gpt-neo-2.7B"
        )
        causal_lm_tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(
            "EleutherAI/gpt-neo-2.7B"
        )
    elif model_type == "mock":
        causal_lm_tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(
            "distilgpt2"
        )
        causal_lm: AutoModelForCausalLM = mockCausalGPT2(causal_lm_tokenizer)
    else:
        causal_lm: AutoModelForCausalLM = AutoModelForCausalLM.from_pretrained(
            "distilgpt2"
        )
        causal_lm_tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(
            "distilgpt2"
        )
    print("Loaded causal LM")
    print(causal_lm)
    causal_lm = causal_lm.to(device)
    # We are setting this token to be eos, so we must make sure to use attention masks
    # to not attend to these positions.
    causal_lm_tokenizer.pad_token_id = causal_lm_tokenizer.eos_token_id
    print("Loaded causal LM to device")

    # load dataset
    # https://www.gutenberg.org/ebooks/71431
    current_path = os.path.dirname(os.path.realpath(__file__))
    textbook_1_path = os.path.join(current_path, "../../", data_file_location)
    dataset_1_loader, seq_len = load_and_format_dataset(
        textbook_1_path,
        causal_lm_tokenizer,
        debug=debug_mode,
        debug_dataset_size=debug_dataset_size,
        training_context_length=training_context_length,
    )
    ## make a pytorch data loader for the dataset

    loss_fn = torch.nn.CrossEntropyLoss()

    experiments = []
    # user_prompt = "Your job is to compress the following text, such that you can reconstruct it later. Do not worry about human legibility and you are allowed to use unicode. Finish with </End compressed text> <example>This is called a covariant transformation law, because the covector components transform by the same matrix as the change of basis matrix. The components of a more general tensor are transformed by some combination of covariant and contravariant transformations, with one transformation law for each index. If the transformation matrix of an index is the inverse matrix of the basis transformation, then the index is called contravariant and is conventionally denoted with an upper index (superscript). If the transformation matrix of an index is the basis transformation itself, then the index is called covariant and is denoted with a lower index (subscript).CovTransLaw:covector=ΔbasisMat. Tensor=comb(cov&contra); 1law/idx. InvMat=basisTrans→contra&↑. BasisTrans=cov&↓</example><Begin text to compress:>"
    user_prompt = f"Create a succinct, compressed version of the following text such that you will be able to reconstruct it verbatim. You can use human legible text, or unicode / non human legible text. Use only {message_context_length} tokens. Reply in only a few words."
    system_prompt = ""
    wandb.config.update({"user_prompt": user_prompt, "system_prompt": system_prompt})
    if experiment_list == "all" or "openai" in experiment_list:
        pass
#         experiments.append(
#             ExperimentConfig(
#                 lambda x: create_openai_helpful_message(
#                     x,
#                     causal_lm_tokenizer,
#                     user_prompt=user_prompt,
#                     system_prompt=system_prompt,
#                     message_context_length=message_context_length,
#                 ),
#                 seq_len,
#                 "openai_helpful_message",
#             )
#         )
#         print("Added openai experiment")
    if experiment_list == "all" or "model_helpful_message" in experiment_list:
        experiments.append(
            ExperimentConfig(
                lambda x: create_model_helpful_message(
                    x,
                    causal_lm_tokenizer,
                    causal_lm,
                    user_prompt=user_prompt,
                    message_context_length=message_context_length,
                ),
                seq_len,
                "model_helpful_message",
            )
        )
    experiments.append(
        ExperimentConfig(
            lambda x: torch.zeros((x.shape[0], 0), dtype=x.dtype, device=x.device),
            seq_len,
            "original",
        )
    )
    if experiment_list == "all" or "helpful_1" in experiment_list:
        experiments.append(
            ExperimentConfig(
                lambda x: create_helpful_message_1(x, message_context_length),
                seq_len,
                "helpful_message_1",
            )
        )
        print("Added helpful message 1 experiment")

    losses_dict = {}
    correct_probs_all_dict = {}
    for experiment in tqdm(experiments, desc="Experiment"):
        losses, correct_probs_all = run_experiment(
            experiment,
            dataset_1_loader,
            causal_lm,
            loss_fn,
            device,
            batched_openai=batched_openai,
            verbose=verbose_output,
        )
        losses_dict[experiment.name] = losses
        correct_probs_all_dict[experiment.name] = correct_probs_all.clone().numpy() / (
            len(dataset_1_loader) * batch_size
        )

    losses_mean = {}
    for exp_name, losses in losses_dict.items():
        print(f"experiment {exp_name} had avg loss of {np.mean(losses)}")
        losses_mean[exp_name] = np.mean(losses)

    # Convert LOGGING_DICT_WANDB to a DataFrame reference
    logging_df = pd.DataFrame(LOGGING_DICT_WANDB)
    # Log the DataFrame to wandb as a table
    wandb.log(
        {
            "Mean Losses": wandb.Table(
                columns=list(losses_mean.keys()), data=[list(losses_mean.values())]
            )
        }
    )
    wandb.log({"LOGGING_DICT_WANDB": wandb.Table(dataframe=logging_df)})

    data_file_name = data_file_location.split(os.path.sep)[-1]
    save_directory = os.path.join(save_directory, f"{model_type}", data_file_name)
    if not os.path.exists(f"{save_directory}"):
        os.makedirs(f"{save_directory}")
    for key in losses_dict:
        losses_dict[key] = [loss.item() for loss in losses_dict[key]]

    wandb.log({"losses_dict": losses_dict})
    df = pd.DataFrame(losses_dict)
    df["batch_index"] = df.index
    df = df.melt(id_vars=["batch_index"], value_vars=list(losses_dict.keys()))
    fig = px.line(df, x="batch_index", y="value", color="variable")
    fig.update_layout(title=f"Losses, batch_size {batch_size}")
    fig.show()
    fig.write_html(f"{save_directory}/losses.html")
    wandb.log({"losses_html": wandb.Plotly(fig)})

    # plot the per token posisions on the same graph
    # normalize the lengths of the different experiments by padding to the max one with zeros
    wandb.log({"correct_probs_all_dict": correct_probs_all_dict})
    max_len = max([len(x) for x in correct_probs_all_dict.values()])
    for exp_name, correct_probs_all in correct_probs_all_dict.items():
        if len(correct_probs_all) < max_len:
            correct_probs_all_dict[exp_name] = np.pad(
                correct_probs_all,
                (0, max_len - len(correct_probs_all)),
                "constant",
                constant_values=0,
            )
    df = pd.DataFrame(correct_probs_all_dict)
    df["position"] = df.index
    df = df.melt(id_vars=["position"], value_vars=list(correct_probs_all_dict.keys()))
    fig = px.line(df, x="position", y="value", color="variable")
    fig.update_layout(title="Probability of correct token at each position")
    fig.show()
    fig.write_html(f"{save_directory}/probability_of_correct_token_at_each_position.html")
    wandb.log({"probability_of_correct_token_at_each_position_html": wandb.Plotly(fig)})

    wandb.config.update({"run_finished_succesfully": True}, allow_val_change=True)
    wandb.finish()


if __name__ == "__main__":
    import fire

    fire.Fire(main)
