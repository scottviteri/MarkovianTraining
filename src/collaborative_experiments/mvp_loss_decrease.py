"""
for llama use
```
torchrun --nproc_per_node 1 src/collaborative_experiments/mvp_loss_decrease.py
```
# Experiment outline:
## Goal:
    1. Measure a decrease in the loss of a model on a particular high quality dataset 
        when a helpful message is prepended to the prompt.
## Hypothesis:
    1. There are strings you can pre-pend to a message that will decrease the loss of the model
    2. Downstream hypothesis if this works: We can see if a LM is able to figure out how to provide such helpful and honest messages.
## Method:
    1. Train a model on a dataset
    2. Measure the loss of the model on each batch of the dataset with and without a prompt
## Evaluation and Experiments:
    1. Look for potential complications by plotting per token loss as a function of token position.
    2. Evaluate null hypothesis: just a random sentence prepended of a fixed length.
    3. Evaluate hypotheses: a) The first sentence from the batch. b) The first sentence wrapped in a message explaining this is a
        helpful message. c) A model generated summary of the sentences. - Helpful message generated by gpt4 as upper bound on competence
"""

import os
import sys
import time
from pathlib import Path
import json

import torch
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from tqdm import tqdm
import accelerate
import plotly.express as px
import pandas as pd
import numpy as np


from collaborative_experiments.constants import MAX_CONTEXT_LENGTH, MSG_CONTEXT_LENGTH

class ExperimentConfig:
    def __init__(self, msg_fn, expected_length, name):
        self.msg_fn = msg_fn
        self.expected_length = expected_length # measured in number of tokens
        self.name = name

def get_device():
    """
    Get's either cuda, cpu, or mps, using accelerate
    """
    accelerator = accelerate.Accelerator()
    device = accelerator.device
    return device

def tile_a_tensor(reshaped_tensor):
    reshaped_tensor.fill_(50)# shape (2, data_context_length)
    reshaped_tensor = reshaped_tensor[0:2]
    for i in range(reshaped_tensor.shape[1] // 3):
        reshaped_tensor[0, i*3 + 1] += 1
        reshaped_tensor[1, i*3 + 1] += 1
        reshaped_tensor[0, i*3 + 2] += 2
        reshaped_tensor[1, i*3 + 2] += 2
    return reshaped_tensor

def load_and_format_dataset(textbook_1_path, causal_lm_tokenizer, debug=False, reduced_data=0):
    data_context_length = MAX_CONTEXT_LENGTH - MSG_CONTEXT_LENGTH
    dataset = load_dataset("text", data_files=textbook_1_path)
    print(dataset)

    # collapse dataset into one big string
    dataset_1 = dataset["train"]
    text = "\n".join(dataset_1["text"])

    # tokenize dataset
    dataset_1_tokenized = causal_lm_tokenizer(text, return_tensors="pt")

    # convert from shape (1, num_tokens) to (num_tokens/1024, 1024)
    tokens_tensor = dataset_1_tokenized['input_ids'].squeeze()
    size = tokens_tensor.shape[0]
    size = (size//data_context_length)*(data_context_length)
    tokens_tensor = tokens_tensor[0:size]
    reshaped_tensor = tokens_tensor.view(-1, data_context_length)
    print(reshaped_tensor.shape)  # Should print torch.Size([num_tokens/data_context_length, data_context_length])
    # turn all values to be the same 11
    if debug: 
        reshaped_tensor = tile_a_tensor(reshaped_tensor)
    elif reduced_data > 0:
        reshaped_tensor = reshaped_tensor[0:reduced_data]

    return reshaped_tensor

def create_helpful_message_1(tokens, tokens_to_grab=MSG_CONTEXT_LENGTH):
    """
    Simply takes a message, grabs the first tokens_to_grab tokens of the data
    and prepends it to the message to make it max context length

    Args:
        tokens (torch.tensor): the tokens to prepend the message to, shape (batch_size, MAX_CONTEXT_LENGTH - MSG_CONTEXT_LENGTH)
        tokens_to_grab (int): the number of tokens to grab from the data as the msg
    Returns
        (torch.tensor): the message with the data prepended
    """
    msg = tokens[:, 0:tokens_to_grab]
    return torch.cat((msg, tokens), dim=1)

def create_helpful_message_2(tokens, tokens_to_grab=MSG_CONTEXT_LENGTH):
    """
    Same as 1 except keeps the shape the same
    """
    msg = tokens[:, 0:tokens_to_grab]
    return torch.cat((msg, tokens[:, 0:-tokens_to_grab]), dim=1)

def train_step(batch, causal_lm, loss_fn, device, correct_probs_all, verbose=False, debug=False, pytest=False):
    # make labels from the batch, one hot encoded of shape (batch_size, seq_len, vocab_size)
    batch = batch.to(device)
    labels = torch.nn.functional.one_hot(batch, num_classes=causal_lm.config.vocab_size).to(torch.float32)
    outputs_original = causal_lm(input_ids=batch.to(device)) # maybe I don't want past_ke_values to be returned? what is that?
    # at this point outputs_original is logits and past_key_values
    logits = outputs_original.logits.detach()#to("cpu")
    loss = loss_fn(logits, labels)
    if verbose: tqdm.write(f"{loss}")

    # Compute softmax over the last dimension to get probabilities
    probs = F.softmax(logits, dim=-1) # shape (batch_size, seq_len, vocab_size)

    # Use gather to pick the probabilities corresponding to the correct token at each position
    correct_probs = probs.gather(-1, batch.unsqueeze(-1)).squeeze(-1).detach() # shape (batch_size, seq_len)
    correct_probs_all += correct_probs.mean(dim=0)
    if debug:
        print("batch: ", batch)
        print("correct probs: ", correct_probs)
        # get one sentence
        # print the tokens that it assigns max probability to
        # print the actual sentence
        sentence_tokens = batch[0]
        sentence_probs = probs[0]
        sentence_correct_probs = correct_probs[0]
    if pytest:
        return loss, logits
    return loss.to("cpu")

def display_results(fname, n_examples, correct_probs_all):
    """
    Deprecated!
    """
    correct_probs_all /= n_examples
    print(correct_probs_all.shape)
    print(correct_probs_all)

    # plot correct_probs_all using plotly
    df = pd.DataFrame(correct_probs_all.numpy())
    df.columns = ["prob"]
    df["position"] = df.index
    fig = px.line(df, x="position", y="prob")
    fig.update_layout(
        title=f"Probability of correct token at each position, function {fname}",
        xaxis_title="Position",
        yaxis_title="Probability",
        font=dict(
            family="Courier New, monospace",
            size=18,
            color="#7f7f7f"
        )
    )
    fig.show()
    # save
    fig.write_html(f"results/{fname}.html")

def run_experiment(config, dataset_1_loader, causal_lm, loss_fn, device):
    correct_probs_all = torch.zeros(config.expected_length).to(device)
    losses = []

    for batch in tqdm(dataset_1_loader, desc=f"Experiment {config.name}"):
        batch = config.msg_fn(batch)
        loss = train_step(batch, causal_lm, loss_fn, device, correct_probs_all)
        losses.append(loss)

    # display_results(config.name, n_examples=len(dataset_1_loader), correct_probs_all=correct_probs_all.to("cpu"))
    return losses, correct_probs_all.to("cpu")

def load_llama_model(
        ckpt_dir: str = "../llama/llama-2-7b",
        tokenizer_path: str = "../llama/tokenizer.model",
        temperature: float = 0.6,
        top_p: float = 0.9,
        max_seq_len: int = 1024,
        max_gen_len: int = 64,
        max_batch_size: int = 8,
        device: str = "mps"
    ):
    os.environ["RANK"] = "0"
    # generator = Llama.build(
    #     ckpt_dir=ckpt_dir,
    #     tokenizer_path=tokenizer_path,
    #     max_seq_len=max_seq_len,
    #     max_batch_size=max_batch_size,
    # ) # fails for strange reasons
    # return generator.model, generator.tokenizer
    start_time = time.time()
    checkpoints = sorted(Path(ckpt_dir).glob("*.pth"))
    for chkpt_path in checkpoints:
        checkpoint = torch.load(chkpt_path, map_location="cpu")
    with open(Path(ckpt_dir) / "params.json", "r") as f:
        params = json.loads(f.read())

    model_args: ModelArgs = ModelArgs(
        max_seq_len=max_seq_len,
        max_batch_size=max_batch_size,
        **params,
    )
    tokenizer = Tokenizer(model_path=tokenizer_path)
    if device == "cuda":
        torch.set_default_tensor_type(torch.cuda.HalfTensor)
    elif device == "mps":
        torch.set_default_tensor_type(torch.HalfTensor)
    else:
        torch.set_default_tensor_type(torch.BFloat16Tensor)
    model = Transformer(model_args) 
    model.load_state_dict(checkpoint, strict=False)
    print(f"Loaded in {time.time() - start_time:.2f} seconds")
    return model, tokenizer

def main(save_dir="results_debug", debug=True, BATCH_SIZE = 1, model_name="gpt-neo", reduced_data=1):
    device = get_device()
    if model_name == "llama":
        from llama import Llama
        from llama.model import ModelArgs, Transformer
        from llama.tokenizer import Tokenizer
        causal_lm, causal_lm_tokenizer = load_llama_model(device=device)
    elif model_name == "gpt2":
        causal_lm = AutoModelForCausalLM.from_pretrained("gpt2")
        causal_lm_tokenizer = AutoTokenizer.from_pretrained("gpt2")
    elif model_name == "gpt-neo":
        causal_lm = AutoModelForCausalLM.from_pretrained("EleutherAI/gpt-neo-2.7B")
        causal_lm_tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neo-2.7B")
    else:
        causal_lm = AutoModelForCausalLM.from_pretrained("distilgpt2")
        causal_lm_tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
    print("Loaded causal LM")
    print(causal_lm)
    causal_lm = causal_lm.to(device)
    print("Loaded causal LM to device")

    # load dataset
    # https://www.gutenberg.org/ebooks/71431
    textbook_1_path = "data/st_patrick_biography.txt"
    reshaped_tensor = load_and_format_dataset(textbook_1_path, causal_lm_tokenizer, debug=debug, reduced_data=reduced_data)
    ## make a pytorch data loader for the dataset
    dataset_1_loader = torch.utils.data.DataLoader(reshaped_tensor, batch_size=BATCH_SIZE, shuffle=True)

    loss_fn = torch.nn.CrossEntropyLoss()

    experiments = []
    experiments.append(ExperimentConfig(lambda x: x, reshaped_tensor.shape[1], "original"))
    experiments.append(ExperimentConfig(create_helpful_message_1, MAX_CONTEXT_LENGTH, "helpful_message_1"))
    experiments.append(ExperimentConfig(create_helpful_message_2, reshaped_tensor.shape[1], "helpful_message_2"))

    losses_dict = {}
    correct_probs_all_dict = {}
    for experiment in tqdm(experiments, desc="Experiment"):
        losses, correct_probs_all = run_experiment(experiment, dataset_1_loader, causal_lm, loss_fn, device)
        losses_dict[experiment.name] = losses
        correct_probs_all_dict[experiment.name] = correct_probs_all.clone().numpy() / (len(dataset_1_loader) * BATCH_SIZE)
    
    for exp_name, losses in losses_dict.items():
        print(f"experiment {exp_name} had avg loss of {np.mean(losses)}")

    if not os.path.exists(f"{save_dir}"):
        os.makedirs(f"{save_dir}")
    # plot the losses on the same graph
    df = pd.DataFrame(losses_dict)
    df["batch_index"] = df.index
    df = df.melt(id_vars=["batch_index"], value_vars=list(losses_dict.keys()))
    fig = px.line(df, x="batch_index", y="value", color="variable")
    fig.update_layout(title=f"Losses, batch_size {BATCH_SIZE}")
    fig.show()
    fig.write_html(f"{save_dir}/losses.html")

    # plot the per token posisions on the same graph
    # normalize the lengths of the different experiments by padding to the max one with zeros
    max_len = max([len(x) for x in correct_probs_all_dict.values()])
    for exp_name, correct_probs_all in correct_probs_all_dict.items():
        if len(correct_probs_all) < max_len:
            correct_probs_all_dict[exp_name] = np.pad(correct_probs_all, (0, max_len - len(correct_probs_all)), "constant", constant_values=0)
    df = pd.DataFrame(correct_probs_all_dict)
    df["position"] = df.index
    df = df.melt(id_vars=["position"], value_vars=list(correct_probs_all_dict.keys()))
    fig = px.line(df, x="position", y="value", color="variable")
    fig.update_layout(title="Probability of correct token at each position")
    fig.show()
    fig.write_html(f"{save_dir}/probability_of_correct_token_at_each_position.html")

if __name__ == "__main__":
    import fire
    fire.Fire(main)