"""
# Experiment outline:
## Goal:
    1. Measure a decrease in the loss of a model on a particular high quality dataset 
        when a helpful message is prepended to the prompt.
## Hypothesis:
    1. There are strings you can pre-pend to a message that will decrease the loss of the model
    2. Downstream hypothesis if this works: We can see if a LM is able to figure out how to provide such helpful and honest messages.
## Method:
    1. Train a model on a dataset
    2. Measure the loss of the model on each batch of the dataset with and without a prompt
## Evaluation and Experiments:
    1. Look for potential complications by plotting per token loss as a function of token position.
    2. Evaluate null hypothesis: just a random sentence prepended of a fixed length.
    3. Evaluate hypotheses: a) The first sentence from the batch. b) The first sentence wrapped in a message explaining this is a
        helpful message. c) A model generated summary of the sentences. - Helpful message generated by gpt4 as upper bound on competence
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
# from llama import Llama
from datasets import load_dataset
import numpy as np

def main():
    causal_lm = AutoModelForCausalLM.from_pretrained("distilgpt2")
    causal_lm_tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
    print("Loaded causal LM")
    print(causal_lm)

    # load dataset
    textbook_1_path = "data/st_patrick_biography.txt"
    dataset = load_dataset("text", data_files=textbook_1_path)
    print(dataset)

    # collapse dataset into one big string
    dataset_1 = dataset["train"]
    text = "\n".join(dataset_1["text"])

    # tokenize dataset
    dataset_1_tokenized = causal_lm_tokenizer(text, return_tensors="pt")

    # convert from shape (1, num_tokens) to (num_tokens/1024, 1024)
    tokens_tensor = dataset_1_tokenized['input_ids'].squeeze()
    size = tokens_tensor.shape[0]
    size = (size//1024)*(1024)
    tokens_tensor = tokens_tensor[0:size]
    reshaped_tensor = tokens_tensor.view(-1, 1024)  # Change 1024 to your desired sequence length

    print(reshaped_tensor.shape)  # Should print torch.Size([num_tokens/1024, 1024])

    # make a pytorch data loader for the dataset
    dataset_1_loader = torch.utils.data.DataLoader(reshaped_tensor, batch_size=1, shuffle=True)

    for batch in dataset_1_loader:
        print(batch.shape)
        break
    


if __name__ == "__main__":
    main()