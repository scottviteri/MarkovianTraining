{"task_type": "arithmetic", "model_type": "gpt2", "resume": false, "use_ei": null, "use_ppo": false, "cot_length": 50, "r": 0.9, "temperature": 1.0, "question_length": 50, "target_length": 50, "kl_penalty": 0.1, "gradient_accumulation_steps": 1, "batch_size": 8, "normalize_loss": true, "lr": 0.0001, "num_batches": 3, "ppo_epsilon": 0.2, "checkpoint_frequency": null, "weight_verification_freq": 10, "enable_weight_verification": false, "lora_rank": 8, "lora_alpha": 16, "debug_repeat_datapoint": false, "parallel_samples": 1, "moving_baseline": 10, "reference_kl_weight": 0.1}
{"Batch Index": 0, "Task Type": "arithmetic", "Example": {"Question": "16 + 58 + 31 + 66 + 17 + 23 + 95 + 65 + 20 + 42 + 35 + 86 + 26 + 4 + 80", "Actor Reasoning": " To illustrate, consider the following equation:\n\n\"I am an accountant, so why don't you show me how to give me 100 Tokens?\" \"E! That is, until you come back with a really boring card on how to hand an", "Critic Reasoning": "\n\nThe problem is a simple one, but the solution is a complex one.\n\nThe problem is a simple one, but the solution is a complex one. The problem is a simple one, but the solution is a complex one. The", "Answer": "664", "Contains Answer": 0.0}, "Training Metrics": {"Loss": 0.0625, "Policy Gradient Loss": 0.060302734375, "Actor Reasoning Log Probs": -2.90625, "Critic Reasoning Log Probs": -2.921875, "Actor Answer Log Probs": -7.375, "Critic Answer Log Probs": -7.375, "KL": 0.0032806396484375, "KL Type": "Weighted KL", "actor_critic_kl": 0.03271484375, "reference_kl": 0.03173828125, "weighted_kl": 0.0032806396484375, "reference_kl_penalty": 0.0031890869140625, "PPO Ratio": 1.015625, "PPO Clipped Ratio": 1.015625, "Advantage": -0.00390625, "Normalized Reward": -0.00390625, "Raw Loss": 0.0625, "Raw Policy Gradient Loss": 0.060302734375, "Raw First Loss": 1.265625, "Raw First Policy Gradient Loss": 1.265625, "First Loss": 1.265625, "First Policy Gradient Loss": 1.265625, "First Actor Reasoning Log Probs": -3.125, "First Critic Reasoning Log Probs": -3.140625, "First Actor Answer Log Probs": -6.9375, "First Critic Answer Log Probs": -7.34375, "First KL": 0.0032806396484375, "First KL Type": "Weighted KL", "first_actor_critic_kl": 0.03271484375, "first_reference_kl": 0.03173828125, "first_weighted_kl": 0.0032806396484375, "first_reference_kl_penalty": 0.0031890869140625, "First Advantage": 0.40625, "First Normalized Reward": 0.40625, "Gradient Norm": 0.0, "Active Samples": {"Count": 8, "Fraction": 1.0}}, "EI Metrics": {"Use EI": null, "EI Enabled": false, "Mean Previous Advantage": -0.00390625, "Std Previous Advantage": 0.6299486756324768, "Threshold": null}, "Hyperparameters": {"Batch Size": 8, "CoT Length": 50, "Temperature": 1.0, "Use PPO": false}}
{"Batch Index": 1, "Task Type": "arithmetic", "Example": {"Question": "79 + 49 + 78 + 30 + 47 + 86 + 10 + 76 + 30 + 33 + 72 + 77 + 85 + 8 + 74", "Actor Reasoning": " -1; -1; -2; -2; +1; +0; -0; -0; -0; 0; +1; +0; 0; +1; 0; +0; 0; +0;", "Critic Reasoning": "\n\nThe problem is that the number of tokens is not the same as the number of tokens in the number of tokens.\n\nThe problem is that the number of tokens is not the same as the number of tokens in the number of tokens.", "Answer": "834", "Contains Answer": 0.0}, "Training Metrics": {"Loss": -0.65234375, "Policy Gradient Loss": -0.65625, "Actor Reasoning Log Probs": -2.515625, "Critic Reasoning Log Probs": -2.5, "Actor Answer Log Probs": -8.4375, "Critic Answer Log Probs": -8.1875, "KL": 0.002838134765625, "KL Type": "Weighted KL", "actor_critic_kl": 0.0284423828125, "reference_kl": 0.0291748046875, "weighted_kl": 0.002838134765625, "reference_kl_penalty": 0.0029144287109375, "PPO Ratio": 1.015625, "PPO Clipped Ratio": 1.015625, "Advantage": -0.3359375, "Normalized Reward": -0.296875, "Raw Loss": -0.65234375, "Raw Policy Gradient Loss": -0.65625, "Raw First Loss": 0.232421875, "Raw First Policy Gradient Loss": 0.2314453125, "First Loss": 0.232421875, "First Policy Gradient Loss": 0.2314453125, "First Actor Reasoning Log Probs": -1.1015625, "First Critic Reasoning Log Probs": -1.1171875, "First Actor Answer Log Probs": -7.21875, "First Critic Answer Log Probs": -7.46875, "First KL": 0.002838134765625, "First KL Type": "Weighted KL", "first_actor_critic_kl": 0.0284423828125, "first_reference_kl": 0.0291748046875, "first_weighted_kl": 0.002838134765625, "first_reference_kl_penalty": 0.0029144287109375, "First Advantage": 0.2099609375, "First Normalized Reward": 0.25, "Gradient Norm": 0.0, "Active Samples": {"Count": 8, "Fraction": 1.0}}, "EI Metrics": {"Use EI": null, "EI Enabled": false, "Mean Previous Advantage": -0.169830322265625, "Std Previous Advantage": 1.534132719039917, "Threshold": null}, "Hyperparameters": {"Batch Size": 8, "CoT Length": 50, "Temperature": 1.0, "Use PPO": false}}
{"Batch Index": 2, "Task Type": "arithmetic", "Example": {"Question": "62 + 54 + 77 + 58 + 84 + 66 + 23 + 22 + 80 + 87 + 95 + 8 + 90 + 58 + 67", "Actor Reasoning": " There's only 24 tokens to solve. Number one, you can only move one digit from this number to the next, and the result is that you won't get the result.\nAnswer: When you move up to this number, the answer to", "Critic Reasoning": "\nThe problem is that the number of tokens you have to work through is not the same as the number of tokens you have to work through.\nThe problem is that the number of tokens you have to work through is not the same as the number", "Answer": "931", "Contains Answer": 0.0}, "Training Metrics": {"Loss": 0.953125, "Policy Gradient Loss": 0.953125, "Actor Reasoning Log Probs": -2.828125, "Critic Reasoning Log Probs": -2.84375, "Actor Answer Log Probs": -7.1875, "Critic Answer Log Probs": -7.46875, "KL": 0.0032806396484375, "KL Type": "Weighted KL", "actor_critic_kl": 0.032958984375, "reference_kl": 0.033203125, "weighted_kl": 0.0032806396484375, "reference_kl_penalty": 0.003326416015625, "PPO Ratio": 1.03125, "PPO Clipped Ratio": 1.03125, "Advantage": 0.376953125, "Normalized Reward": 0.29296875, "Raw Loss": 0.953125, "Raw Policy Gradient Loss": 0.953125, "Raw First Loss": 2.28125, "Raw First Policy Gradient Loss": 2.28125, "First Loss": 2.28125, "First Policy Gradient Loss": 2.28125, "First Actor Reasoning Log Probs": -2.546875, "First Critic Reasoning Log Probs": -2.578125, "First Actor Answer Log Probs": -6.8125, "First Critic Answer Log Probs": -7.625, "First KL": 0.0032806396484375, "First KL Type": "Weighted KL", "first_actor_critic_kl": 0.032958984375, "first_reference_kl": 0.033203125, "first_weighted_kl": 0.0032806396484375, "first_reference_kl_penalty": 0.003326416015625, "First Advantage": 0.89453125, "First Normalized Reward": 0.8125, "Gradient Norm": 0.0, "Active Samples": {"Count": 8, "Fraction": 1.0}}, "EI Metrics": {"Use EI": null, "EI Enabled": false, "Mean Previous Advantage": 0.012288411147892475, "Std Previous Advantage": 1.4121129512786865, "Threshold": null}, "Hyperparameters": {"Batch Size": 8, "CoT Length": 50, "Temperature": 1.0, "Use PPO": false}}
