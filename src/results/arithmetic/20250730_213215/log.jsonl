{"task_type": "arithmetic", "model_type": "gpt2", "resume": false, "use_ei": null, "use_ppo": false, "cot_length": 50, "r": 0.9, "temperature": 1.0, "question_length": 50, "target_length": 50, "kl_penalty": 0.1, "gradient_accumulation_steps": 1, "batch_size": 32, "normalize_loss": true, "lr": 0.0001, "num_batches": 3, "ppo_epsilon": 0.2, "checkpoint_frequency": null, "weight_verification_freq": 10, "enable_weight_verification": false, "lora_rank": 8, "lora_alpha": 16, "debug_repeat_datapoint": false, "parallel_samples": 1, "moving_baseline": 10, "reference_kl_weight": 0.1}
{"Batch Index": 0, "Task Type": "arithmetic", "Example": {"Question": "5 + 28 + 15 + 19 + 93 + 13 + 7 + 79 + 30 + 3 + 97 + 93 + 68 + 56 + 25", "Actor Reasoning": " Your own numbers? Question: 27 + 43 + 56 + 17 + 29 + 56 + 20 + 28 + 32 + 45 + 35 + 42 + 37 + 29\nProof of concept: The math is a bit tedious. To set it up,", "Critic Reasoning": "\n\nThe problem is that the number of tokens is a function of the number of tokens in the solution.\n\nThe problem is that the number of tokens is a function of the number of tokens in the solution. The number of tokens is a", "Answer": "631", "Contains Answer": 0.0}, "Training Metrics": {"Loss": 0.6484375, "Policy Gradient Loss": 0.64453125, "Actor Reasoning Log Probs": -2.5, "Critic Reasoning Log Probs": -2.515625, "Actor Answer Log Probs": -8.25, "Critic Answer Log Probs": -8.4375, "KL": 0.002899169921875, "KL Type": "Weighted KL", "PPO Ratio": 1.03125, "PPO Clipped Ratio": 1.03125, "Advantage": 0.2294921875, "Normalized Reward": 0.2294921875, "Raw Loss": 0.6484375, "Raw Policy Gradient Loss": 0.64453125, "Raw First Loss": 1.1796875, "Raw First Policy Gradient Loss": 1.171875, "First Loss": 1.1796875, "First Policy Gradient Loss": 1.171875, "First Actor Reasoning Log Probs": -2.6875, "First Critic Reasoning Log Probs": -2.71875, "First Actor Answer Log Probs": -6.375, "First Critic Answer Log Probs": -6.8125, "First KL": 0.00390625, "First KL Type": "Weighted KL", "First Advantage": 0.4375, "First Normalized Reward": 0.4375, "Gradient Norm": 0.0, "Active Samples": {"Count": 32, "Fraction": 1.0}}, "EI Metrics": {"Use EI": null, "EI Enabled": false, "Mean Previous Advantage": 0.2294921875, "Std Previous Advantage": 0.9863141179084778, "Threshold": null}, "Hyperparameters": {"Batch Size": 32, "CoT Length": 50, "Temperature": 1.0, "Use PPO": false}}
{"Batch Index": 1, "Task Type": "arithmetic", "Example": {"Question": "31 + 85 + 24 + 36 + 62 + 41 + 52 + 72 + 5 + 99 + 82 + 17 + 13 + 33 + 34", "Actor Reasoning": " the problem is a problem of algebraic geometry. In this case the math problem refers towards the number between 0 and 3^3, but you don't even need to worry about rounding, you just have to deal with the question of rounding to check", "Critic Reasoning": "\nThe problem is that the number of tokens is not the same as the number of tokens in the number of tokens in the number of tokens in the number of tokens in the number of tokens in the number of tokens in the number of tokens in the", "Answer": "686", "Contains Answer": 0.0}, "Training Metrics": {"Loss": 0.0128173828125, "Policy Gradient Loss": 0.01055908203125, "Actor Reasoning Log Probs": -2.75, "Critic Reasoning Log Probs": -2.765625, "Actor Answer Log Probs": -7.5, "Critic Answer Log Probs": -7.84375, "KL": 0.0030364990234375, "KL Type": "Weighted KL", "PPO Ratio": 0.96875, "PPO Clipped Ratio": 0.96875, "Advantage": -0.00048828125, "Normalized Reward": 0.35546875, "Raw Loss": 0.0128173828125, "Raw Policy Gradient Loss": 0.01055908203125, "Raw First Loss": -0.81640625, "Raw First Policy Gradient Loss": -0.82421875, "First Loss": -0.81640625, "First Policy Gradient Loss": -0.82421875, "First Actor Reasoning Log Probs": -2.8125, "First Critic Reasoning Log Probs": -2.78125, "First Actor Answer Log Probs": -7.0, "First Critic Answer Log Probs": -7.0625, "First KL": 0.002838134765625, "First KL Type": "Weighted KL", "First Advantage": -0.29296875, "First Normalized Reward": 0.0625, "Gradient Norm": 0.0, "Active Samples": {"Count": 32, "Fraction": 1.0}}, "EI Metrics": {"Use EI": null, "EI Enabled": false, "Mean Previous Advantage": 0.114501953125, "Std Previous Advantage": 1.0212756395339966, "Threshold": null}, "Hyperparameters": {"Batch Size": 32, "CoT Length": 50, "Temperature": 1.0, "Use PPO": false}}
{"Batch Index": 2, "Task Type": "arithmetic", "Example": {"Question": "48 + 58 + 1 + 28 + 28 + 36 + 55 + 38 + 98 + 93 + 61 + 81 + 8 + 86 + 11", "Actor Reasoning": " For us, you would have a new language as that project's extension of Rust language.\nYour new language is the language of choice. You could apply for the position for 10 months, until you arrive in 2016 - if your team is able to", "Critic Reasoning": "\nThe problem is that the number of tokens is not the same as the number of tokens in the number of tokens.\nThe problem is that the number of tokens is not the same as the number of tokens in the number of tokens.\nThe", "Answer": "730", "Contains Answer": 0.0}, "Training Metrics": {"Loss": -0.93359375, "Policy Gradient Loss": -0.9375, "Actor Reasoning Log Probs": -2.921875, "Critic Reasoning Log Probs": -2.9375, "Actor Answer Log Probs": -7.75, "Critic Answer Log Probs": -7.875, "KL": 0.0034027099609375, "KL Type": "Weighted KL", "PPO Ratio": 1.015625, "PPO Clipped Ratio": 1.015625, "Advantage": -0.302734375, "Normalized Reward": 0.142578125, "Raw Loss": -0.93359375, "Raw Policy Gradient Loss": -0.9375, "Raw First Loss": 1.5390625, "Raw First Policy Gradient Loss": 1.5390625, "First Loss": 1.5390625, "First Policy Gradient Loss": 1.5390625, "First Actor Reasoning Log Probs": -3.578125, "First Critic Reasoning Log Probs": -3.59375, "First Actor Answer Log Probs": -6.6875, "First Critic Answer Log Probs": -7.5625, "First KL": 0.0034637451171875, "First KL Type": "Weighted KL", "First Advantage": 0.4296875, "First Normalized Reward": 0.875, "Gradient Norm": 0.0, "Active Samples": {"Count": 32, "Fraction": 1.0}}, "EI Metrics": {"Use EI": null, "EI Enabled": false, "Mean Previous Advantage": -0.02472686767578125, "Std Previous Advantage": 0.9828262329101562, "Threshold": null}, "Hyperparameters": {"Batch Size": 32, "CoT Length": 50, "Temperature": 1.0, "Use PPO": false}}
